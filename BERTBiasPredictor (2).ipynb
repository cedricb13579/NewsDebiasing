{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from simplediff import diff\n",
    "\n",
    "\n",
    "## TAKEN FROM GITHUB CODE OF NEUTRALIZING BIAS\n",
    "## https://github.com/rpryzant/neutralizing-bias\n",
    "\n",
    "# from https://spacy.io/api/annotation#section-dependency-parsing\n",
    "RELATIONS = [\n",
    "    'det', 'amod', 'nsubj', 'prep', 'pobj', 'ROOT', \n",
    "    'attr', 'punct', 'advmod', 'compound', 'acl', 'agent', \n",
    "    'aux', 'ccomp', 'dobj', 'cc', 'conj', 'appos', 'nsubjpass', \n",
    "    'auxpass', 'poss', 'nummod', 'nmod', 'relcl', 'mark', \n",
    "    'advcl', 'pcomp', 'npadvmod', 'preconj', 'neg', 'xcomp', \n",
    "    'csubj', 'prt', 'parataxis', 'expl', 'case', 'acomp', 'predet',\n",
    "    'quantmod', 'dep', 'oprd', 'intj', 'dative', 'meta', 'csubjpass', \n",
    "    '<UNK>'\n",
    "]\n",
    "REL2ID = {x: i for i, x in enumerate(RELATIONS)}\n",
    "\n",
    "# from https://spacy.io/api/annotation#section-pos-tagging\n",
    "POS_TAGS = [\n",
    "    'DET', 'ADJ', 'NOUN', 'ADP', 'NUM', 'VERB', 'PUNCT', 'ADV', \n",
    "    'PART', 'CCONJ', 'PRON', 'X', 'INTJ', 'PROPN', 'SYM',\n",
    "    '<UNK>'\n",
    "]\n",
    "POS2ID = {x: i for i, x in enumerate(POS_TAGS)}\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "class Featurizer:\n",
    "\n",
    "    def __init__(self, tok2id={}, pad_id=0, lexicon_feature_bits=1):\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {x: tok for tok, x in tok2id.items()}\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.pos2id = POS2ID\n",
    "        self.rel2id = REL2ID\n",
    "\n",
    "        self.lexicons = {\n",
    "            'assertives': self.read_lexicon('lexicons/assertives_hooper1975.txt'),\n",
    "            'entailed_arg': self.read_lexicon('lexicons/entailed_arg_berant2012.txt'),\n",
    "            'entailed': self.read_lexicon('lexicons/entailed_berant2012.txt'), \n",
    "            'entailing_arg': self.read_lexicon('lexicons/entailing_arg_berant2012.txt'), \n",
    "            'entailing': self.read_lexicon('lexicons/entailing_berant2012.txt'), \n",
    "            'factives': self.read_lexicon('lexicons/factives_hooper1975.txt'),\n",
    "            'hedges': self.read_lexicon('lexicons/hedges_hyland2005.txt'),\n",
    "            'implicatives': self.read_lexicon('lexicons/implicatives_karttunen1971.txt'),\n",
    "            'negatives': self.read_lexicon('lexicons/negative_liu2005.txt'),\n",
    "            'positives': self.read_lexicon('lexicons/positive_liu2005.txt'),\n",
    "            'npov': self.read_lexicon('lexicons/npov_lexicon.txt'),\n",
    "            'reports': self.read_lexicon('lexicons/report_verbs.txt'),\n",
    "            'strong_subjectives': self.read_lexicon('lexicons/strong_subjectives_riloff2003.txt'),\n",
    "            'weak_subjectives': self.read_lexicon('lexicons/weak_subjectives_riloff2003.txt')\n",
    "        }\n",
    "        self.lexicon_feature_bits = lexicon_feature_bits\n",
    "\n",
    "\n",
    "    def get_feature_names(self):\n",
    "\n",
    "        lexicon_feature_names = list(self.lexicons.keys())\n",
    "        context_feature_names = [x + '_context' for x in lexicon_feature_names]\n",
    "        pos_names = list(list(zip(*sorted(self.pos2id.items(), key=lambda x: x[1])))[0])\n",
    "        rel_names = list(list(zip(*sorted(self.rel2id.items(), key=lambda x: x[1])))[0])\n",
    "\n",
    "        return lexicon_feature_names + context_feature_names + pos_names + rel_names        \n",
    "\n",
    "    def read_lexicon(self, fp):\n",
    "        out = set([\n",
    "            l.strip() for l in open(fp, errors='ignore') \n",
    "            if not l.startswith('#') and not l.startswith(';')\n",
    "            and len(l.strip().split()) == 1\n",
    "        ])\n",
    "        return out\n",
    "\n",
    "\n",
    "    def lexicon_features(self, words, bits=2):\n",
    "        assert bits in [1, 2]\n",
    "        if bits == 1:\n",
    "            true = 1\n",
    "            false = 0\n",
    "        else:\n",
    "            true = [1, 0]\n",
    "            false = [0, 1]\n",
    "    \n",
    "        out = []\n",
    "        for word in words:\n",
    "            out.append([\n",
    "                true if word in lexicon else false \n",
    "                for _, lexicon in self.lexicons.items()\n",
    "            ])\n",
    "        out = np.array(out)\n",
    "\n",
    "        if bits == 2:\n",
    "            out = out.reshape(len(words), -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def context_features(self, lex_feats, window_size=2):\n",
    "        out = []\n",
    "        nwords = lex_feats.shape[0]\n",
    "        nfeats = lex_feats.shape[1]\n",
    "        for wi in range(lex_feats.shape[0]):\n",
    "            window_start = max(wi - window_size, 0)\n",
    "            window_end = min(wi + window_size + 1, nwords)\n",
    "\n",
    "            left = lex_feats[window_start: wi, :] if wi > 0 else np.zeros((1, nfeats))\n",
    "            right = lex_feats[wi + 1: window_end, :] if wi < nwords - 1 else np.zeros((1, nfeats))\n",
    "\n",
    "            out.append((np.sum(left + right, axis=0) > 0).astype(int))\n",
    "\n",
    "        return np.array(out)\n",
    "\n",
    "\n",
    "    def features(self, id_seq, rel_ids, pos_ids):\n",
    "        if self.pad_id in id_seq:\n",
    "            pad_idx = id_seq.index(self.pad_id)\n",
    "            pad_len = len(id_seq[pad_idx:])\n",
    "            id_seq = id_seq[:pad_idx]\n",
    "            rel_ids = rel_ids[:pad_idx]\n",
    "            pos_ids = pos_ids[:pad_idx]\n",
    "        else:\n",
    "            pad_len = 0\n",
    "\n",
    "        toks = [self.id2tok[x] for x in id_seq]\n",
    "        # build list of [word, [tok indices the word came from]]\n",
    "        words = []\n",
    "        word_indices = []\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.startswith('##'):\n",
    "                words[-1] += tok.replace('##', '')\n",
    "                word_indices[-1].append(i)\n",
    "            else:\n",
    "                words.append(tok)\n",
    "                word_indices.append([i])\n",
    "\n",
    "        # get expert features\n",
    "        lex_feats = self.lexicon_features(words, bits=self.lexicon_feature_bits)\n",
    "        context_feats = self.context_features(lex_feats)\n",
    "        expert_feats = np.concatenate((lex_feats, context_feats), axis=1)\n",
    "        # break word-features into tokens\n",
    "        feats = np.concatenate([\n",
    "            np.repeat(np.expand_dims(word_vec, axis=0), len(indices), axis=0) \n",
    "            for (word_vec, indices) in zip(expert_feats, word_indices)\n",
    "        ], axis=0)\n",
    "\n",
    "        # add in the pos and relational features\n",
    "        pos_feats = np.zeros((len(pos_ids), len(POS2ID)))\n",
    "        pos_feats[range(len(pos_ids)), pos_ids] = 1\n",
    "        rel_feats = np.zeros((len(rel_ids), len(REL2ID)))\n",
    "        rel_feats[range(len(rel_ids)), rel_ids] = 1\n",
    "        \n",
    "        feats = np.concatenate((feats, pos_feats, rel_feats), axis=1)\n",
    "\n",
    "        # add pad back in                \n",
    "        feats = np.concatenate((feats, np.zeros((pad_len, feats.shape[1]))))\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "    def featurize_batch(self, batch_ids, rel_ids, pos_ids, padded_len=0):\n",
    "        \"\"\" takes [batch, len] returns [batch, len, features] \"\"\"\n",
    "        #print(rel_ids)\n",
    "        #print(batch_ids)\n",
    "        #print(pos_ids)\n",
    "        batch_feats = [\n",
    "            self.features(list(id_seq), list(rel_ids), list(pos_ids)) \n",
    "            for id_seq, rel_ids, pos_ids in zip(batch_ids, rel_ids, pos_ids)]\n",
    "        batch_feats = np.array(batch_feats)\n",
    "        return batch_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok2id = tokenizer.vocab\n",
    "tok2id['<del>'] = len(tok2id)\n",
    "bertmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 12\n",
    "hidden_size = 768\n",
    "\n",
    "## TAKEN FROM GITHUB CODE OF NEUTRALIZING BIAS\n",
    "## https://github.com/rpryzant/neutralizing-bias\n",
    "\n",
    "class ConcatCombine(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, feature_size, out_size, layers,\n",
    "            dropout_prob, small=False, pre_enrich=False, activation=False,\n",
    "            include_categories=False, category_emb=False,\n",
    "            add_category_emb=False):\n",
    "        super(ConcatCombine, self).__init__()\n",
    "\n",
    "        self.include_categories = include_categories\n",
    "        self.add_category_emb = add_category_emb\n",
    "        if include_categories:\n",
    "            if category_emb and not add_category_emb:\n",
    "                feature_size *= 2\n",
    "            elif not category_emb:\n",
    "                feature_size += 43\n",
    "\n",
    "        if layers == 1:\n",
    "            self.out = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size + feature_size, out_size),\n",
    "                torch.nn.Dropout(dropout_prob))\n",
    "        elif layers == 2:\n",
    "            waist_size = min(hidden_size, feature_size) if small else max(hidden_size, feature_size)\n",
    "            if activation:\n",
    "                self.out = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_size + feature_size, waist_size),\n",
    "                    torch.nn.Dropout(dropout_prob),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(waist_size, out_size),\n",
    "                    torch.nn.Dropout(dropout_prob))\n",
    "            else:\n",
    "                self.out = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_size + feature_size, waist_size),\n",
    "                    torch.nn.Dropout(dropout_prob),\n",
    "                    torch.nn.Linear(waist_size, out_size),\n",
    "                    torch.nn.Dropout(dropout_prob))\n",
    "        if pre_enrich:\n",
    "            if activation:\n",
    "                self.enricher = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(feature_size, feature_size),\n",
    "                    torch.nn.ReLU())\n",
    "            else:\n",
    "                self.enricher = torch.nn.Linear(feature_size, feature_size)\n",
    "        else:\n",
    "            self.enricher = None\n",
    "        # manually set cuda because module doesn't see these combiners for bottom \n",
    "#         if CUDA:\n",
    "#             self.out = self.out.cuda()\n",
    "#             if self.enricher: \n",
    "#                 self.enricher = self.enricher.cuda()\n",
    "                \n",
    "    def forward(self, hidden, features, categories=None):\n",
    "        if self.include_categories:\n",
    "            categories = categories.unsqueeze(1)\n",
    "            categories = categories.repeat(1, features.shape[1], 1)\n",
    "            if self.add_category_emb:\n",
    "                features = features + categories\n",
    "            else:\n",
    "                features = torch.cat((features, categories), -1)\n",
    "\n",
    "        if self.enricher is not None:\n",
    "            features = self.enricher(features)\n",
    "\n",
    "        return self.out(torch.cat((hidden, features), dim=-1))\n",
    "    \n",
    "\n",
    "class BertDetector(torch.nn.Module):\n",
    "    def __init__(self,cls_num_labels=2,token_num_labels=2,tok2id=None):\n",
    "        super(BertDetector,self).__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.featurizer = Featurizer(tok2id,lexicon_feature_bits=1)\n",
    "        self.cls_dropout = torch.nn.Dropout(0.15)\n",
    "        self.cls_classifier = torch.nn.Linear(hidden_size,cls_num_labels)\n",
    "        \n",
    "        self.token_dropout = torch.nn.Dropout(0.15)\n",
    "        #self.token_classifier = torch.nn.Linear(hidden_size,token_num_labels)\n",
    "        \n",
    "        self.token_classifier = ConcatCombine(\n",
    "                hidden_size, 90, token_num_labels, \n",
    "                1, 0, False, pre_enrich=False,\n",
    "                activation=False,\n",
    "                include_categories=False,\n",
    "                category_emb=False,\n",
    "                add_category_emb=False)\n",
    "        \n",
    "    def forward(self,input_ids,train=None,token_type_ids=None,attention_mask=None, \n",
    "        labels=None,rel_ids=None,pos_ids=None,categories=None,pre_len=None):\n",
    "        features = torch.tensor(self.featurizer.featurize_batch(\n",
    "            input_ids.numpy(), \n",
    "            rel_ids.numpy(), \n",
    "            pos_ids.numpy(), \n",
    "            padded_len=input_ids.shape[1]),dtype=torch.float)\n",
    "        results = self.bert(input_ids)\n",
    "        sequence = results.last_hidden_state\n",
    "        pooled = results.pooler_output\n",
    "        if train == \"train\":\n",
    "            cls_logits = self.cls_dropout(self.cls_classifier(pooled))\n",
    "            token_logits = self.token_dropout(self.token_classifier(sequence,features))\n",
    "        else:\n",
    "            cls_logits = self.cls_classifier(pooled)\n",
    "            token_logits = self.token_classifier(sequence,features)\n",
    "        return cls_logits,token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2859, 18152,  2006,  9317,  2000,  1000,  2954,  2067,  1000,\n",
       "           2044,  1996,  2142,  2163,  2623,  1037,  2825,  2117,  2461,  1997,\n",
       "          23234, 21857,  2015,  2006,  1002,  3263,  4551,  4276,  1997,  2822,\n",
       "           5350,  1012,  1996,  1057,  1012,  1055,  1012,  6378,  2207,  2006,\n",
       "           9857,  2443,  3445,  7773,  2006, 10964,  2833,  3688,  1998,  7325,\n",
       "           8139,  1012,   102]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " tensor([45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
       "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
       "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45]),\n",
       " tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = \"China vowed on Wednesday to \\\"fight back\\\" after the United States announced a possible second round of tariff hikes on $200 billion worth of Chinese goods. The U.S. proposal released on Tuesday included increased taxes on imported food products and consumer electronics.\"\n",
    "text_2 = \"not sure\"\n",
    "indexed_tokens = tokenizer.encode(text_1, add_special_tokens=True)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "try: \n",
    "    segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "except:\n",
    "    segments_ids = [0] * (lengths[0] + 1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "segments_tensors\n",
    "rel_tensor = torch.tensor([REL2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "pos_tensor = torch.tensor([POS2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "\n",
    "(tokens_tensor,segments_tensors,rel_tensor,pos_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.0517, -0.5244]], grad_fn=<AddmmBackward>),\n",
       "  tensor([[[ 0.4846,  0.2814],\n",
       "           [ 0.3863,  0.0817],\n",
       "           [ 0.2775, -0.3158],\n",
       "           [ 0.5077, -0.1507],\n",
       "           [ 0.1739, -0.0308],\n",
       "           [-0.1166, -0.1629],\n",
       "           [ 0.0812, -0.3776],\n",
       "           [-0.2618, -0.6185],\n",
       "           [-0.0981, -0.3988],\n",
       "           [ 0.4846,  0.0514],\n",
       "           [ 0.4730, -0.0056],\n",
       "           [ 0.5697, -0.3963],\n",
       "           [ 0.5234, -0.2881],\n",
       "           [ 0.6422, -0.1745],\n",
       "           [ 0.3207, -0.4543],\n",
       "           [ 0.1040, -0.0873],\n",
       "           [-0.1369,  0.3556],\n",
       "           [-0.0221, -0.0388],\n",
       "           [-0.2431, -0.0989],\n",
       "           [ 0.0042, -0.2524],\n",
       "           [ 0.0117, -0.7513],\n",
       "           [-0.3080, -0.4874],\n",
       "           [-0.1520, -0.2115],\n",
       "           [ 0.0286, -0.0834],\n",
       "           [ 0.3734,  0.0252],\n",
       "           [ 0.1491,  0.0577],\n",
       "           [-0.0652,  0.1527],\n",
       "           [-0.1598, -0.0655],\n",
       "           [-0.3144, -0.3606],\n",
       "           [-0.1854, -0.1669],\n",
       "           [-0.3078, -0.2365],\n",
       "           [ 0.2207,  0.0671],\n",
       "           [ 0.5071, -0.2909],\n",
       "           [ 0.4759, -0.3023],\n",
       "           [ 0.2181,  0.0423],\n",
       "           [ 0.6091, -0.3323],\n",
       "           [ 0.1995,  0.0727],\n",
       "           [ 0.0611, -0.1239],\n",
       "           [ 0.5594, -0.0643],\n",
       "           [ 0.6788,  0.0845],\n",
       "           [ 0.1484, -0.0612],\n",
       "           [-0.1129, -0.3460],\n",
       "           [-0.3384, -0.2683],\n",
       "           [-0.0618, -0.2910],\n",
       "           [-0.3479, -0.2780],\n",
       "           [-0.2129, -0.3252],\n",
       "           [-0.1205, -0.4727],\n",
       "           [-0.3907, -0.1863],\n",
       "           [-0.3228, -0.2458],\n",
       "           [-0.0745, -0.3513],\n",
       "           [-0.2199, -0.1914],\n",
       "           [ 0.2341,  0.1343],\n",
       "           [ 0.0539, -0.1572]]], grad_fn=<AddBackward0>)),\n",
       " tensor([[[0.0266, 0.0290],\n",
       "          [0.0241, 0.0238],\n",
       "          [0.0216, 0.0160],\n",
       "          [0.0272, 0.0188],\n",
       "          [0.0195, 0.0212],\n",
       "          [0.0146, 0.0186],\n",
       "          [0.0177, 0.0150],\n",
       "          [0.0126, 0.0118],\n",
       "          [0.0148, 0.0147],\n",
       "          [0.0266, 0.0230],\n",
       "          [0.0263, 0.0218],\n",
       "          [0.0289, 0.0147],\n",
       "          [0.0276, 0.0164],\n",
       "          [0.0311, 0.0184],\n",
       "          [0.0225, 0.0139],\n",
       "          [0.0182, 0.0201],\n",
       "          [0.0143, 0.0312],\n",
       "          [0.0160, 0.0211],\n",
       "          [0.0128, 0.0198],\n",
       "          [0.0164, 0.0170],\n",
       "          [0.0166, 0.0103],\n",
       "          [0.0120, 0.0134],\n",
       "          [0.0141, 0.0177],\n",
       "          [0.0168, 0.0201],\n",
       "          [0.0238, 0.0224],\n",
       "          [0.0190, 0.0232],\n",
       "          [0.0153, 0.0255],\n",
       "          [0.0139, 0.0205],\n",
       "          [0.0119, 0.0153],\n",
       "          [0.0136, 0.0185],\n",
       "          [0.0120, 0.0173],\n",
       "          [0.0204, 0.0234],\n",
       "          [0.0272, 0.0164],\n",
       "          [0.0263, 0.0162],\n",
       "          [0.0203, 0.0228],\n",
       "          [0.0301, 0.0157],\n",
       "          [0.0200, 0.0235],\n",
       "          [0.0174, 0.0193],\n",
       "          [0.0286, 0.0205],\n",
       "          [0.0323, 0.0238],\n",
       "          [0.0190, 0.0206],\n",
       "          [0.0146, 0.0155],\n",
       "          [0.0117, 0.0167],\n",
       "          [0.0154, 0.0164],\n",
       "          [0.0116, 0.0166],\n",
       "          [0.0132, 0.0158],\n",
       "          [0.0145, 0.0136],\n",
       "          [0.0111, 0.0182],\n",
       "          [0.0118, 0.0171],\n",
       "          [0.0152, 0.0154],\n",
       "          [0.0131, 0.0181],\n",
       "          [0.0207, 0.0250],\n",
       "          [0.0173, 0.0187]]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertDetector(tok2id=tok2id)\n",
    "results = model(tokens_tensor,token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "(results,torch.nn.Softmax(dim=1)(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 10381, 10626, 11253,  2953,  2213,  1000,  1996,  8382,  2166,\n",
       "           3736,  6299,  1000,  2019,  3720,  2012,  4345,  2118,  4346,  5875,\n",
       "           8866,  2055, 10381, 10626, 11253,  2953,  2213,  1012,   102, 10381,\n",
       "          10626, 11253,  2953,  2213,  1000,  1996,  8382,  2166,  3736,  6299,\n",
       "           1000,  2019,  3720,  2012,  4345,  2118,  4346,  8866,  2055, 10381,\n",
       "          10626, 11253,  2953,  2213,  1012,   102]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " tensor([ 5,  5,  5,  5,  5,  7,  0,  1, 14, 14, 14,  7,  0, 17,  3,  9,  4, 10,\n",
       "          1, 14,  3,  4,  4,  4,  4,  4,  7,  5,  5,  5,  5,  5,  7,  0,  1, 14,\n",
       "         14, 14,  7,  0, 17,  3,  9,  4, 10,  1, 14,  3,  4,  4,  4,  4,  4,  7,\n",
       "          0,  0]),\n",
       " tensor([2, 2, 2, 2, 2, 6, 0, 1, 2, 2, 2, 6, 0, 2, 3, 2, 2, 5, 1, 2, 3, 2, 2, 2,\n",
       "         2, 2, 6, 2, 2, 2, 2, 2, 6, 0, 1, 2, 2, 2, 6, 0, 2, 3, 2, 2, 5, 1, 2, 3,\n",
       "         2, 2, 2, 2, 2, 6, 0, 0]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test= \"165188319\tch ##lor ##of ##or ##m \\\" the molecular life ##sa ##ver \\\" an article at oxford university providing interesting facts about ch ##lor ##of ##or ##m .\tch ##lor ##of ##or ##m \\\" the molecular life ##sa ##ver \\\" an article at oxford university providing facts about ch ##lor ##of ##or ##m .\tchloroform \\\"the molecular lifesaver\\\" an article at oxford university providing interesting facts about chloroform.\tchloroform \\\"the molecular lifesaver\\\" an article at oxford university providing facts about chloroform.\tNOUN NOUN NOUN NOUN NOUN PUNCT DET ADJ NOUN NOUN NOUN PUNCT DET NOUN ADP NOUN NOUN VERB ADJ NOUN ADP NOUN NOUN NOUN NOUN NOUN PUNCT\tROOT ROOT ROOT ROOT ROOT punct det amod dobj dobj dobj punct det appos prep compound pobj acl amod dobj prep pobj pobj pobj pobj pobj punct\"\n",
    "[revid, _, _, biased, nonbiased, pos, rels] = text_test.strip().split(\"\\t\")\n",
    "\n",
    "indexed_tokens = tokenizer.encode(biased.strip(),nonbiased.strip(), add_special_tokens=True)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "try: \n",
    "    segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "except:\n",
    "    segments_ids = [0] * (lengths[0] + 1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "segments_tensors\n",
    "rel_tensor = torch.tensor([REL2ID[x] for x in rels.strip().split(\" \")]*2)\n",
    "pos_tensor = torch.tensor([POS2ID[x] for x in pos.strip().split(\" \")]*2)\n",
    "while rel_tensor.size()[0] < len(indexed_tokens):\n",
    "    rel_tensor = torch.cat((rel_tensor, torch.tensor([0])), dim=-1)\n",
    "while pos_tensor.size()[0] < len(indexed_tokens):\n",
    "    pos_tensor = torch.cat((pos_tensor, torch.tensor([0])), dim=-1)\n",
    "    \n",
    "ground_truth_list = [(x[0],len(x[1])) for x in diff(biased.strip().split(),nonbiased.strip().split())]\n",
    "ground_truth = []\n",
    "for i in ground_truth_list:\n",
    "    if i[0] == \"-\":\n",
    "        ground_truth.extend([1]*i[1])\n",
    "    else:\n",
    "        ground_truth.extend([0]*i[1])\n",
    "ground_truth\n",
    "#rel_tensor = torch.tensor([REL2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "#pos_tensor = torch.tensor([POS2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "\n",
    "(tokens_tensor,segments_tensors,rel_tensor,pos_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('=',\n",
       "  ['chloroform',\n",
       "   '\"the',\n",
       "   'molecular',\n",
       "   'lifesaver\"',\n",
       "   'an',\n",
       "   'article',\n",
       "   'at',\n",
       "   'oxford',\n",
       "   'university',\n",
       "   'providing']),\n",
       " ('-', ['interesting']),\n",
       " ('=', ['facts', 'about', 'chloroform.'])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(biased.strip().split(),nonbiased.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 56]), torch.Size([1, 56]), torch.Size([56]), torch.Size([56]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokens_tensor.size(),segments_tensors.size(),rel_tensor.size(),pos_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((tensor([[ 0.1812, -0.2723]], grad_fn=<AddmmBackward>),\n",
       "  tensor([[[-0.6615,  0.1716],\n",
       "           [-0.2087,  0.2696],\n",
       "           [-0.4593, -0.0786],\n",
       "           [ 0.1960, -0.6619],\n",
       "           [-0.5898,  0.0152],\n",
       "           [-0.4499, -0.0224],\n",
       "           [-0.1283, -0.2362],\n",
       "           [-0.4974, -0.1788],\n",
       "           [-0.3294, -0.4225],\n",
       "           [-0.0648, -0.2173],\n",
       "           [ 0.4301, -0.2815],\n",
       "           [ 0.2340, -0.3602],\n",
       "           [-0.0687, -0.1605],\n",
       "           [ 0.0427, -0.3235],\n",
       "           [-0.0901, -0.3714],\n",
       "           [ 0.2663, -0.3083],\n",
       "           [-0.1482, -0.1498],\n",
       "           [ 0.1569,  0.2401],\n",
       "           [ 0.2422, -0.3405],\n",
       "           [-0.0475, -0.4424],\n",
       "           [-0.1028, -0.2259],\n",
       "           [-0.1633, -0.5786],\n",
       "           [ 0.0893, -0.5959],\n",
       "           [-0.2626, -0.3703],\n",
       "           [ 0.3370, -0.8774],\n",
       "           [-0.4027, -0.2592],\n",
       "           [-0.3603, -0.5741],\n",
       "           [-0.4923, -0.6132],\n",
       "           [ 0.2358, -0.2823],\n",
       "           [ 0.0046, -0.4434],\n",
       "           [-0.4095, -0.2791],\n",
       "           [ 0.2277, -0.8269],\n",
       "           [-0.4807, -0.1023],\n",
       "           [-0.3124, -0.3060],\n",
       "           [-0.0072, -0.3047],\n",
       "           [-0.4991, -0.2885],\n",
       "           [-0.3108, -0.5494],\n",
       "           [-0.0888, -0.3238],\n",
       "           [ 0.3778, -0.3260],\n",
       "           [ 0.3088, -0.4921],\n",
       "           [-0.0711, -0.1983],\n",
       "           [-0.0010, -0.4895],\n",
       "           [-0.1173, -0.3777],\n",
       "           [ 0.1572, -0.3328],\n",
       "           [-0.1209, -0.1191],\n",
       "           [ 0.2884, -0.0039],\n",
       "           [ 0.1352, -0.4840],\n",
       "           [-0.2012, -0.4621],\n",
       "           [-0.1878, -0.6386],\n",
       "           [ 0.0397, -0.5529],\n",
       "           [-0.2702, -0.5007],\n",
       "           [ 0.3350, -0.9238],\n",
       "           [-0.3866, -0.2817],\n",
       "           [-0.3982, -0.6091],\n",
       "           [-0.5005, -0.7034],\n",
       "           [ 0.2811, -0.3215]]], grad_fn=<AddBackward0>)),\n",
       " tensor([[[0.0098, 0.0291],\n",
       "          [0.0154, 0.0321],\n",
       "          [0.0120, 0.0227],\n",
       "          [0.0230, 0.0126],\n",
       "          [0.0105, 0.0249],\n",
       "          [0.0121, 0.0240],\n",
       "          [0.0166, 0.0194],\n",
       "          [0.0115, 0.0205],\n",
       "          [0.0136, 0.0161],\n",
       "          [0.0177, 0.0197],\n",
       "          [0.0291, 0.0185],\n",
       "          [0.0239, 0.0171],\n",
       "          [0.0177, 0.0209],\n",
       "          [0.0197, 0.0177],\n",
       "          [0.0173, 0.0169],\n",
       "          [0.0247, 0.0180],\n",
       "          [0.0163, 0.0211],\n",
       "          [0.0221, 0.0312],\n",
       "          [0.0241, 0.0174],\n",
       "          [0.0180, 0.0158],\n",
       "          [0.0171, 0.0196],\n",
       "          [0.0161, 0.0137],\n",
       "          [0.0207, 0.0135],\n",
       "          [0.0145, 0.0169],\n",
       "          [0.0265, 0.0102],\n",
       "          [0.0126, 0.0189],\n",
       "          [0.0132, 0.0138],\n",
       "          [0.0116, 0.0133],\n",
       "          [0.0240, 0.0185],\n",
       "          [0.0190, 0.0157],\n",
       "          [0.0126, 0.0185],\n",
       "          [0.0238, 0.0107],\n",
       "          [0.0117, 0.0221],\n",
       "          [0.0138, 0.0181],\n",
       "          [0.0188, 0.0181],\n",
       "          [0.0115, 0.0184],\n",
       "          [0.0139, 0.0142],\n",
       "          [0.0173, 0.0177],\n",
       "          [0.0276, 0.0177],\n",
       "          [0.0258, 0.0150],\n",
       "          [0.0176, 0.0201],\n",
       "          [0.0189, 0.0150],\n",
       "          [0.0168, 0.0168],\n",
       "          [0.0221, 0.0176],\n",
       "          [0.0168, 0.0218],\n",
       "          [0.0252, 0.0244],\n",
       "          [0.0217, 0.0151],\n",
       "          [0.0155, 0.0154],\n",
       "          [0.0157, 0.0129],\n",
       "          [0.0197, 0.0141],\n",
       "          [0.0144, 0.0149],\n",
       "          [0.0264, 0.0097],\n",
       "          [0.0129, 0.0185],\n",
       "          [0.0127, 0.0133],\n",
       "          [0.0115, 0.0121],\n",
       "          [0.0251, 0.0178]]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "          0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "          0, 0, 1, 0, 1, 1, 1, 0]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertDetector(tok2id=tok2id)\n",
    "results = model(tokens_tensor,token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "(results,torch.nn.Softmax(dim=1)(results[1]),torch.argmax(torch.nn.Softmax(dim=1)(results[1]),dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 1.        , 0.4240568 , 1.        ,\n",
       "        1.        , 0.8977299 , 1.        , 0.9110744 , 0.8585662 ,\n",
       "        0.49084106, 0.5519678 , 0.91231334, 0.69338435, 0.7548474 ,\n",
       "        0.56288207, 0.9984426 , 1.        , 0.55840206, 0.67377377,\n",
       "        0.88414544, 0.660167  , 0.5039606 , 0.8979253 , 0.296886  ,\n",
       "        1.        , 0.80752534, 0.8861072 , 0.5956413 , 0.6389143 ,\n",
       "        1.        , 0.3483299 , 1.        , 1.        , 0.7427272 ,\n",
       "        1.        , 0.78771454, 0.7905991 , 0.49470252, 0.44892636,\n",
       "        0.8805472 , 0.61353207, 0.7707484 , 0.61257994, 1.        ,\n",
       "        0.74654776, 0.5383597 , 0.7703232 , 0.6371283 , 0.55286795,\n",
       "        0.794155  , 0.28399527, 1.        , 0.8098489 , 0.8163168 ,\n",
       "        0.5473603 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(results[1].detach())[:,:,:2]\n",
    "x = x - x.max(axis=2, keepdims=True)\n",
    "y = np.exp(x)\n",
    "y / y.sum(axis=2, keepdims=True)\n",
    "y[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 10\n",
      "tensor(0.3227, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 20\n",
      "tensor(0.3706, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 30\n",
      "tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 40\n",
      "tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 50\n",
      "tensor(0.3349, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 60\n",
      "tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 70\n",
      "tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 80\n",
      "tensor(0.3312, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 90\n",
      "tensor(0.3135, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 100\n",
      "tensor(0.3315, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 110\n",
      "tensor(0.3234, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 120\n",
      "tensor(0.3249, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 130\n",
      "tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 140\n",
      "tensor(0.3231, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 150\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 160\n",
      "tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 170\n",
      "tensor(0.3513, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 180\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 190\n",
      "tensor(0.3398, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 200\n",
      "tensor(0.3262, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 210\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 220\n",
      "tensor(0.3216, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 230\n",
      "tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 240\n",
      "tensor(0.3134, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 260\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 270\n",
      "tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 280\n",
      "tensor(0.3274, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 290\n",
      "tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 300\n",
      "tensor(0.3186, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 320\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 330\n",
      "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 340\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 350\n",
      "tensor(0.3306, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 370\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 380\n",
      "tensor(0.3267, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 390\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 400\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 410\n",
      "tensor(0.3489, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 420\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 430\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 440\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 460\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 470\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 480\n",
      "tensor(0.3319, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 490\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 500\n",
      "tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 510\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 520\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 530\n",
      "tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 540\n",
      "tensor(0.3239, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 550\n",
      "tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 560\n",
      "tensor(0.3236, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 570\n",
      "tensor(0.3260, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 580\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 590\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 600\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 610\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 630\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 640\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 650\n",
      "tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 660\n",
      "tensor(0.3242, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 670\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 700\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 710\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 720\n",
      "tensor(0.3269, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 730\n",
      "tensor(0.3274, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 740\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 750\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 760\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 770\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 780\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 790\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 800\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 810\n",
      "tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 830\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 840\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 850\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 860\n",
      "tensor(0.3456, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 870\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 880\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 890\n",
      "tensor(0.3196, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 900\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 910\n",
      "tensor(0.3276, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 920\n",
      "tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 930\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 940\n",
      "tensor(0.3414, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 950\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 970\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 980\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1000\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1010\n",
      "tensor(0.3195, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1020\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1030\n",
      "tensor(0.3288, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1050\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1060\n",
      "tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1070\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1080\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1090\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1100\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1110\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1120\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1130\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1140\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1150\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1160\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1170\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1180\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1190\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1200\n",
      "tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1210\n",
      "tensor(0.3437, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1220\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1230\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1240\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1260\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1270\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1280\n",
      "tensor(0.3226, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1290\n",
      "tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1320\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1330\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1340\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1350\n",
      "tensor(0.3418, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1360\n",
      "tensor(0.3245, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1370\n",
      "tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1380\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1390\n",
      "tensor(0.3236, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1400\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1410\n",
      "tensor(0.3340, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1420\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1430\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1440\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1460\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1480\n",
      "tensor(0.3495, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1490\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1500\n",
      "tensor(0.3533, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1510\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1520\n",
      "tensor(0.3210, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1530\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1540\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1550\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1560\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1570\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1580\n",
      "tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1590\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1600\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1610\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1620\n",
      "tensor(0.3250, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1630\n",
      "tensor(0.3276, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1640\n",
      "tensor(0.3414, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1650\n",
      "tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1660\n",
      "tensor(0.3288, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1670\n",
      "tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1690\n",
      "tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1700\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1710\n",
      "tensor(0.3201, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1720\n",
      "tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1730\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1740\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1750\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1760\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1770\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1780\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1790\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1800\n",
      "tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1810\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1820\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1830\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1840\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1850\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1860\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1870\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1880\n",
      "tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1890\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1900\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1910\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1920\n",
      "tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1930\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1940\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1950\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1970\n",
      "tensor(0.3187, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1980\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 1990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2000\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2010\n",
      "tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2030\n",
      "tensor(0.3302, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2040\n",
      "tensor(0.3266, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2050\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2060\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2070\n",
      "tensor(0.3205, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2080\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2090\n",
      "tensor(0.3214, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2100\n",
      "tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2110\n",
      "tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2120\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2130\n",
      "tensor(0.3194, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2140\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2150\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2160\n",
      "tensor(0.3283, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2170\n",
      "tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2180\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2190\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2200\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2210\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2220\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2230\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2240\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2260\n",
      "tensor(0.3363, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2270\n",
      "tensor(0.3243, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2280\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2290\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2320\n",
      "tensor(0.3191, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2330\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2340\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2350\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2370\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2380\n",
      "tensor(0.3191, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2390\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2400\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2410\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2420\n",
      "tensor(0.3180, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2430\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2440\n",
      "tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2450\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2460\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2480\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2490\n",
      "tensor(0.3533, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2500\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2510\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2530\n",
      "tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2540\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2550\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2560\n",
      "tensor(0.3593, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2570\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2580\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2590\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2600\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2610\n",
      "tensor(0.3189, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2620\n",
      "tensor(0.3253, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2630\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2640\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2650\n",
      "tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2660\n",
      "tensor(0.3302, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2670\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2700\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2710\n",
      "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2720\n",
      "tensor(0.3252, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2730\n",
      "tensor(0.3336, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2740\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2750\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2760\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2770\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2780\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2790\n",
      "tensor(0.3555, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2800\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2810\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2830\n",
      "tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2840\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2850\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2860\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2870\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2880\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2890\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2900\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2910\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2920\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2930\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2940\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2950\n",
      "tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2970\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2980\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 2990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3000\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3010\n",
      "tensor(0.3193, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3030\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3050\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3060\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3070\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3080\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3090\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3100\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3110\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3120\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3130\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3140\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3150\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3160\n",
      "tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3170\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3180\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3190\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3200\n",
      "tensor(0.3271, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3210\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3220\n",
      "tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3230\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3240\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3250\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3260\n",
      "tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3270\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3280\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3290\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3320\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3330\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3340\n",
      "tensor(0.3195, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3350\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3370\n",
      "tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3380\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3390\n",
      "tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3400\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3410\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3420\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3430\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3440\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3460\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3470\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3480\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3490\n",
      "tensor(0.3196, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3500\n",
      "tensor(0.3352, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3510\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3530\n",
      "tensor(0.3289, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3540\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3550\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3560\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3570\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3580\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3590\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3600\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3610\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3630\n",
      "tensor(0.3183, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3640\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3650\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3660\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3670\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3680\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3700\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3710\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3720\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3730\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3740\n",
      "tensor(0.3639, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3750\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3760\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3770\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3780\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3790\n",
      "tensor(0.3327, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3800\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3810\n",
      "tensor(0.3309, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3820\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3830\n",
      "tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3840\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3850\n",
      "tensor(0.3204, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3860\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3870\n",
      "tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3880\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3890\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3900\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3910\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3920\n",
      "tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3930\n",
      "tensor(0.3282, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3940\n",
      "tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3950\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3970\n",
      "tensor(0.3555, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3980\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 3990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4000\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4010\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4030\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4040\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4050\n",
      "tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4060\n",
      "tensor(0.3214, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4070\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4080\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4090\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4100\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4110\n",
      "tensor(0.3337, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4120\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4130\n",
      "tensor(0.3187, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4140\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4150\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4160\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4170\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4180\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4190\n",
      "tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4200\n",
      "tensor(0.3500, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4210\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4220\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4230\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4240\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4260\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4270\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4280\n",
      "tensor(0.3201, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4290\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4320\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4330\n",
      "tensor(0.3571, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4340\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4350\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4360\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4370\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4380\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4390\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4400\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4410\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4420\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4430\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4440\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4450\n",
      "tensor(0.3219, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4460\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4480\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4490\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4500\n",
      "tensor(0.3567, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4510\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4530\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4540\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4550\n",
      "tensor(0.3295, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4560\n",
      "tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4570\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4580\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4590\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4600\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4610\n",
      "tensor(0.3301, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4630\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4640\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4650\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4660\n",
      "tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4670\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4700\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4710\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4720\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4730\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4740\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4750\n",
      "tensor(0.3327, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4760\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4770\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4780\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4790\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4800\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4810\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4830\n",
      "tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4840\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4850\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4860\n",
      "tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4870\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4880\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4890\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4900\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4910\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4920\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4930\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4940\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4950\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4970\n",
      "tensor(0.3271, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4980\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 4990\n",
      "tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5000\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5010\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5020\n",
      "tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5030\n",
      "tensor(0.3513, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5050\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5060\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5070\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5080\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5090\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5100\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5110\n",
      "tensor(0.3196, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5120\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5130\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5140\n",
      "tensor(0.3184, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5150\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5160\n",
      "tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5170\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5180\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5190\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5200\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5210\n",
      "tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5220\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5230\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5240\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5260\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5270\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5280\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5290\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5300\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5310\n",
      "tensor(0.3181, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5320\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5330\n",
      "tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5340\n",
      "tensor(0.3184, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5350\n",
      "tensor(0.3207, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5370\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5380\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5390\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5400\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5410\n",
      "tensor(0.3358, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5420\n",
      "tensor(0.3194, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5430\n",
      "tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5440\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5460\n",
      "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5480\n",
      "tensor(0.3190, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5490\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5500\n",
      "tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5510\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5530\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5540\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5550\n",
      "tensor(0.3254, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5560\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5570\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5580\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5590\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5600\n",
      "tensor(0.3318, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5610\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5630\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5640\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5650\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5660\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5670\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5680\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5700\n",
      "tensor(0.3213, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5710\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5720\n",
      "tensor(0.3326, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5730\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5740\n",
      "tensor(0.3489, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5750\n",
      "tensor(0.3213, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5760\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5770\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5780\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5790\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5800\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5810\n",
      "tensor(0.3327, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5830\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5840\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5850\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5860\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5870\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5880\n",
      "tensor(0.3253, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5890\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5900\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5910\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5920\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5930\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5940\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5950\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5960\n",
      "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5970\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5980\n",
      "tensor(0.3183, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 5990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6000\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6010\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6030\n",
      "tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6050\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6060\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6070\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6080\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6090\n",
      "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6100\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6110\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6120\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6130\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6140\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6150\n",
      "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6160\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6170\n",
      "tensor(0.3363, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6180\n",
      "tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6190\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6200\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6210\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6220\n",
      "tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6230\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6240\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6260\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6270\n",
      "tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6280\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6290\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6300\n",
      "tensor(0.3213, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6310\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6320\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6330\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6340\n",
      "tensor(0.3200, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6350\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6370\n",
      "tensor(0.3478, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6380\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6390\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6400\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6410\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6420\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6430\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6440\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6460\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6480\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6490\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6500\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6510\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6530\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6540\n",
      "tensor(0.3188, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6550\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6560\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6570\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6580\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6590\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6600\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6610\n",
      "tensor(0.3494, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6630\n",
      "tensor(0.3257, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6640\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6650\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6660\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6670\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6690\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6700\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6710\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6720\n",
      "tensor(0.3330, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6730\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6740\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6750\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6760\n",
      "tensor(0.3183, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6770\n",
      "tensor(0.3175, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6780\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6790\n",
      "tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6800\n",
      "tensor(0.3197, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6810\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6830\n",
      "tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6840\n",
      "tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6850\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6860\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6870\n",
      "tensor(0.3456, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6880\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6890\n",
      "tensor(0.3240, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6900\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6910\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6920\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6930\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6940\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6950\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6970\n",
      "tensor(0.3189, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6980\n",
      "tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 6990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7000\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7010\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7030\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7050\n",
      "tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7060\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7070\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7080\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7090\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7100\n",
      "tensor(0.3171, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7110\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7120\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7130\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7140\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7150\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7160\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7170\n",
      "tensor(0.3187, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7180\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7190\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7200\n",
      "tensor(0.3329, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7210\n",
      "tensor(0.3219, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7220\n",
      "tensor(0.3213, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7230\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7240\n",
      "tensor(0.3207, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7250\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7260\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7270\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7280\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7290\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7320\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7330\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7340\n",
      "tensor(0.3433, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7350\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7360\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7370\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7380\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7390\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7400\n",
      "tensor(0.3198, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7410\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7420\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7430\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7440\n",
      "tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7460\n",
      "tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7480\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7490\n",
      "tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7500\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7510\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7530\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7540\n",
      "tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7550\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7560\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7570\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7580\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7590\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7600\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7610\n",
      "tensor(0.3273, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7630\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7640\n",
      "tensor(0.3277, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7650\n",
      "tensor(0.3204, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7660\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7670\n",
      "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7680\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7700\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7710\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7720\n",
      "tensor(0.3352, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7730\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7740\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7750\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7760\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7770\n",
      "tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7780\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7790\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7800\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7810\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7830\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7840\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7850\n",
      "tensor(0.3314, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7860\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7870\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7880\n",
      "tensor(0.3246, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7890\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7900\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7910\n",
      "tensor(0.3196, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7920\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7930\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7940\n",
      "tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7950\n",
      "tensor(0.3494, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7960\n",
      "tensor(0.3404, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7970\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7980\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 7990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8000\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8010\n",
      "tensor(0.3217, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8030\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8050\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8060\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8070\n",
      "tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8080\n",
      "tensor(0.3250, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8090\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8100\n",
      "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8110\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8120\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8130\n",
      "tensor(0.3185, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8140\n",
      "tensor(0.3255, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8150\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8160\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8170\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8180\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8190\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8200\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8210\n",
      "tensor(0.3192, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8220\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8230\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8240\n",
      "tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8250\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8260\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8270\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8280\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8290\n",
      "tensor(0.3338, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8320\n",
      "tensor(0.3327, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8330\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8340\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8350\n",
      "tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8370\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8380\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8390\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8400\n",
      "tensor(0.3639, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8410\n",
      "tensor(0.3177, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8420\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8430\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8440\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8450\n",
      "tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8460\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8470\n",
      "tensor(0.3201, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8480\n",
      "tensor(0.3186, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8490\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8500\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8510\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8530\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8540\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8550\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8560\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8570\n",
      "tensor(0.3223, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8580\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8590\n",
      "tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8600\n",
      "tensor(0.3204, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8610\n",
      "tensor(0.3206, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8630\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8640\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8650\n",
      "tensor(0.3295, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8660\n",
      "tensor(0.3470, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8670\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8700\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8710\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8720\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8730\n",
      "tensor(0.3177, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8740\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8750\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8760\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8770\n",
      "tensor(0.3241, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8780\n",
      "tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8790\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8800\n",
      "tensor(0.3425, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8810\n",
      "tensor(0.3170, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8820\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8830\n",
      "tensor(0.3219, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8840\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8850\n",
      "tensor(0.3494, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8860\n",
      "tensor(0.3607, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8870\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8880\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8890\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8900\n",
      "tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8910\n",
      "tensor(0.3291, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8920\n",
      "tensor(0.3221, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8930\n",
      "tensor(0.3244, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8940\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8950\n",
      "tensor(0.3414, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8970\n",
      "tensor(0.3212, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8980\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 8990\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9000\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9010\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9020\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9030\n",
      "tensor(0.3219, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9040\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9050\n",
      "tensor(0.3378, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9060\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9070\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9080\n",
      "tensor(0.3478, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9090\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9100\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9110\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9120\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9130\n",
      "tensor(0.3251, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9140\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9150\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9160\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9170\n",
      "tensor(0.3386, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9180\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9190\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9200\n",
      "tensor(0.3204, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9210\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9220\n",
      "tensor(0.3264, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9230\n",
      "tensor(0.3233, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9240\n",
      "tensor(0.3350, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9250\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9260\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9270\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9280\n",
      "tensor(0.3228, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9290\n",
      "tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9300\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9310\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9320\n",
      "tensor(0.3209, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9330\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9340\n",
      "tensor(0.3580, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9350\n",
      "tensor(0.3395, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9360\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9370\n",
      "tensor(0.3276, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9380\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9390\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9400\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9410\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9420\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9430\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9440\n",
      "tensor(0.3235, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9450\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9460\n",
      "tensor(0.3206, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9470\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9480\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9490\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9500\n",
      "tensor(0.3449, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9510\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9520\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9530\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9540\n",
      "tensor(0.3418, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9550\n",
      "tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9560\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9570\n",
      "tensor(0.3259, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9580\n",
      "tensor(0.3335, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9590\n",
      "tensor(0.3298, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9600\n",
      "tensor(0.3203, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9610\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9620\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9630\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9640\n",
      "tensor(0.3238, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9650\n",
      "tensor(0.3305, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9660\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9670\n",
      "tensor(0.3215, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9680\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9690\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9700\n",
      "tensor(0.3202, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9710\n",
      "tensor(0.3270, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9720\n",
      "tensor(0.3196, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9730\n",
      "tensor(0.3225, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9740\n",
      "tensor(0.3356, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9750\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9760\n",
      "tensor(0.3248, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9770\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9780\n",
      "tensor(0.3323, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9790\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9800\n",
      "tensor(0.3370, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9810\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9820\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9830\n",
      "tensor(0.3230, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9840\n",
      "tensor(0.3229, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9850\n",
      "tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9860\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9870\n",
      "tensor(0.3268, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9880\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9890\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9900\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9910\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9920\n",
      "tensor(0.3316, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9930\n",
      "tensor(0.3279, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9940\n",
      "tensor(0.3285, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9950\n",
      "tensor(0.3411, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9960\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9970\n",
      "tensor(0.3344, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9980\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 9990\n",
      "tensor(0.3333, grad_fn=<NllLossBackward>)\n",
      " \n",
      "Line: 10000\n",
      "tensor(0.3133, grad_fn=<NllLossBackward>)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "f = open(\"biased.word.train\",encoding=\"utf-8\")\n",
    "i = 0\n",
    "model = BertDetector(tok2id=tok2id)\n",
    "weights = torch.ones(2)\n",
    "weights[-1] = 0\n",
    "celossfn = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "softmax = torch.nn.Softmax(dim=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "for line in f:\n",
    "    linedata = line.strip().split('\\t')\n",
    "    if len(linedata) == 7:\n",
    "        [revid, biased2, nonbiased2, biased, nonbiased, pos, rels] = linedata\n",
    "        indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "    elif len(linedata) == 5:\n",
    "        [revid, biased2, nonbiased2, biased, nonbiased] = linedata\n",
    "        indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "        pos = [\"<UNK>\"]*len(indexed_tokens)\n",
    "        rels = [\"<UNK>\"]*len(indexed_tokens)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "    try: \n",
    "        segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "    except:\n",
    "        segments_ids = [0] * (lengths[0] + 1)\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    rel_tensor = torch.tensor([REL2ID[x] for x in rels.strip().split(\" \")])\n",
    "    pos_tensor = torch.tensor([POS2ID[x] for x in pos.strip().split(\" \")])\n",
    "    while rel_tensor.size()[0] < len(indexed_tokens):\n",
    "        rel_tensor = torch.cat((rel_tensor, torch.tensor([0])), dim=-1)\n",
    "    while pos_tensor.size()[0] < len(indexed_tokens):\n",
    "        pos_tensor = torch.cat((pos_tensor, torch.tensor([0])), dim=-1)\n",
    "\n",
    "    ground_truth_list = [(x[0],len(x[1])) for x in diff(biased2.strip().split(),nonbiased2.strip().split())]\n",
    "    ground_truth = []\n",
    "    for j in ground_truth_list:\n",
    "        if j[0] == \"-\":\n",
    "            ground_truth.extend([1]*j[1])\n",
    "        elif j[0] == \"=\":\n",
    "            ground_truth.extend([0]*j[1])\n",
    "    \n",
    "    #print((tokens_tensor,segments_tensors,rel_tensor,pos_tensor))\n",
    "    i = i + 1\n",
    "    \n",
    "    results = model(tokens_tensor,train=\"train\",token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "    _,token_logits = results\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    while len(ground_truth) + 2 < len(softmax(token_logits)[0]):\n",
    "        ground_truth.extend([0])\n",
    "    loss = celossfn(softmax(token_logits)[0],torch.Tensor([0] + ground_truth + [0]).type('torch.LongTensor'))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(\"Line: \" + str(i))\n",
    "        print(loss)\n",
    "        print(\" \")\n",
    "        #break\n",
    "    if i >= 10000:\n",
    "        break\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1390,  0.3761]], grad_fn=<AddmmBackward>),\n",
       " tensor([[[ 12.0613, -12.1638],\n",
       "          [ 12.0665, -12.1632],\n",
       "          [ 12.0435, -12.1683],\n",
       "          [ 12.0458, -12.1656],\n",
       "          [ 12.0983, -12.1359],\n",
       "          [ 12.0951, -12.1393],\n",
       "          [ 12.0457, -12.1650],\n",
       "          [ 12.0444, -12.1664],\n",
       "          [ 12.0646, -12.1652],\n",
       "          [ 12.0678, -12.2047],\n",
       "          [ 12.0684, -12.2040],\n",
       "          [ 12.0507, -12.1675],\n",
       "          [ 12.0654, -12.2072],\n",
       "          [ 12.0650, -12.2054],\n",
       "          [ 12.0662, -12.1632],\n",
       "          [ 12.0502, -12.1740],\n",
       "          [ 12.0277, -12.1280],\n",
       "          [ 12.0552, -12.0866],\n",
       "          [ 12.0875, -12.2047],\n",
       "          [ 12.0487, -12.0863],\n",
       "          [ 12.0463, -12.1089],\n",
       "          [ 12.0672, -12.1571],\n",
       "          [ 12.0672, -12.1629],\n",
       "          [ 12.0668, -12.1635],\n",
       "          [ 12.0625, -12.1621],\n",
       "          [ 12.0531, -12.1570]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linedata = line.strip().split('\\t')\n",
    "# if len(linedata) == 7:\n",
    "#     [revid, biased2, nonbiased2, biased, nonbiased, pos, rels] = linedata\n",
    "#     indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "# elif len(linedata) == 5:\n",
    "#     [revid, biased2, nonbiased2, biased, nonbiased] = linedata\n",
    "#     indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "#     pos = [\"<UNK>\"]*len(indexed_tokens)\n",
    "#     rels = [\"<UNK>\"]*len(indexed_tokens)\n",
    "# else:\n",
    "#     continue\n",
    "biased2 = \"one of the stand ##out tracks , \\\" parker ' s band , \\\" was a tribute to legendary jazz saxophonist charlie parker .\"\n",
    "indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "pos = \" \".join([\"<UNK>\"]*len(indexed_tokens))\n",
    "rels = \" \".join([\"<UNK>\"]*len(indexed_tokens))\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "try: \n",
    "    segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "except:\n",
    "    segments_ids = [0] * (lengths[0] + 1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "rel_tensor = torch.tensor([REL2ID[x] for x in rels.strip().split(\" \")])\n",
    "pos_tensor = torch.tensor([POS2ID[x] for x in pos.strip().split(\" \")])\n",
    "while rel_tensor.size()[0] < len(indexed_tokens):\n",
    "    rel_tensor = torch.cat((rel_tensor, torch.tensor([0])), dim=-1)\n",
    "while pos_tensor.size()[0] < len(indexed_tokens):\n",
    "    pos_tensor = torch.cat((pos_tensor, torch.tensor([0])), dim=-1)\n",
    "\n",
    "# ground_truth_list = [(x[0],len(x[1])) for x in diff(biased2.strip().split(),nonbiased2.strip().split())]\n",
    "# ground_truth = []\n",
    "# for j in ground_truth_list:\n",
    "#     if j[0] == \"-\":\n",
    "#         ground_truth.extend([1]*j[1])\n",
    "#     elif j[0] == \"=\":\n",
    "#         ground_truth.extend([0]*j[1])\n",
    "\n",
    "#print((tokens_tensor,segments_tensors,rel_tensor,pos_tensor))\n",
    "i = i + 1\n",
    "\n",
    "results = model(tokens_tensor,train=\"test\",token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "_,token_logits = results\n",
    "\n",
    "#optimizer.zero_grad()\n",
    "#while len(ground_truth) + 2 < len(softmax(token_logits)[0]):\n",
    "#    ground_truth.extend([0])\n",
    "#loss = celossfn(softmax(token_logits)[0],torch.Tensor([0] + ground_truth + [0]).type('torch.LongTensor'))\n",
    "#loss.backward()\n",
    "#optimizer.step()\n",
    "#    print(loss)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,\n",
       " 3.2985076e-11,\n",
       " array([3.0005255e-11, 3.0547957e-11, 3.0557164e-11, 2.9871293e-11,\n",
       "        2.9863261e-11, 3.0578387e-11, 3.0578446e-11, 3.0000447e-11,\n",
       "        2.8747331e-11, 2.8750840e-11, 3.0352058e-11, 2.8743492e-11,\n",
       "        2.8807269e-11, 3.0010980e-11, 3.0170186e-11, 3.2307317e-11,\n",
       "        3.2761797e-11, 2.8185124e-11, 3.2985076e-11, 3.2325809e-11,\n",
       "        3.0165814e-11, 2.9989691e-11, 2.9985001e-11, 3.0156839e-11],\n",
       "       dtype=float32),\n",
       " array([0.37919772, 0.49226162, 0.49417993, 0.35128862, 0.34961534,\n",
       "        0.49860138, 0.49861366, 0.3781959 , 0.11712753, 0.11785865,\n",
       "        0.45144895, 0.11632774, 0.12961477, 0.38039035, 0.4135586 ,\n",
       "        0.8587986 , 0.9534829 , 0.        , 1.        , 0.86265117,\n",
       "        0.41264784, 0.3759552 , 0.37497795, 0.41077796], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = softmax(token_logits)[:,:,1][0][1:-1].detach().numpy()\n",
    "idx = np.argmax(index)\n",
    "(idx, np.max(index), index, (index-np.min(index))/(np.max(index)-np.min(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "legendary\n",
      "tribute\n",
      "jazz\n",
      "a\n",
      ",\n",
      "tracks\n",
      "the\n",
      "of\n",
      "s\n",
      "was\n",
      "saxophonist\n",
      ".\n",
      "\"\n",
      "one\n",
      "\"\n",
      "charlie\n",
      "parker\n",
      "stand\n",
      "##out\n",
      ",\n",
      "'\n",
      "parker\n",
      "band\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "for idx in np.flip(np.argsort(index)):\n",
    "    print(tokenizer.decode(indexed_tokens[idx+1:idx+2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1522,  0.3634]], grad_fn=<AddmmBackward>),\n",
       " tensor([[[ 12.0286, -12.1308],\n",
       "          [ 12.0710, -12.2217],\n",
       "          [ 12.0445, -12.1258],\n",
       "          [ 12.0460, -12.1254],\n",
       "          [ 12.0155, -12.1856],\n",
       "          [ 12.0115, -12.1400],\n",
       "          [ 12.0649, -12.1095],\n",
       "          [ 12.0625, -12.2352],\n",
       "          [ 12.0035, -12.1505],\n",
       "          [ 12.0664, -12.1544],\n",
       "          [ 12.0925, -12.1856],\n",
       "          [ 12.0838, -12.1191],\n",
       "          [ 12.0535, -12.1156],\n",
       "          [ 12.1451, -12.3043],\n",
       "          [ 12.0966, -12.0914],\n",
       "          [ 12.0921, -12.0971],\n",
       "          [ 12.0614, -12.1898],\n",
       "          [ 12.0815, -12.2083],\n",
       "          [ 12.0926, -12.1787],\n",
       "          [ 12.0928, -12.1681],\n",
       "          [ 12.0925, -12.1683],\n",
       "          [ 12.0231, -12.1738],\n",
       "          [ 12.0195, -12.1750],\n",
       "          [ 12.0579, -12.1409],\n",
       "          [ 12.0627, -12.2069],\n",
       "          [ 12.0266, -12.1727],\n",
       "          [ 12.0362, -12.1191],\n",
       "          [ 12.0782, -12.1594],\n",
       "          [ 12.0559, -12.1112],\n",
       "          [ 12.0184, -12.1603],\n",
       "          [ 12.0619, -12.2157],\n",
       "          [ 12.1324, -12.1659],\n",
       "          [ 12.0654, -12.2105],\n",
       "          [ 12.0273, -12.1766],\n",
       "          [ 12.0353, -12.1216],\n",
       "          [ 12.0595, -12.1663],\n",
       "          [ 12.0444, -12.1456],\n",
       "          [ 12.0323, -12.1543],\n",
       "          [ 12.0963, -12.3016],\n",
       "          [ 12.0658, -12.2263],\n",
       "          [ 12.1019, -12.1638],\n",
       "          [ 11.9671, -12.1710],\n",
       "          [ 12.0307, -12.1680],\n",
       "          [ 12.0204, -12.1761],\n",
       "          [ 12.0811, -12.1477],\n",
       "          [ 12.0409, -12.1910],\n",
       "          [ 12.0488, -12.1506]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "testset = pd.read_csv(\"newdataset_withBias.csv\")[\"text\"].to_numpy()\n",
    "# linedata = line.strip().split('\\t')\n",
    "# if len(linedata) == 7:\n",
    "#     [revid, biased2, nonbiased2, biased, nonbiased, pos, rels] = linedata\n",
    "#     indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "# elif len(linedata) == 5:\n",
    "#     [revid, biased2, nonbiased2, biased, nonbiased] = linedata\n",
    "#     indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "#     pos = [\"<UNK>\"]*len(indexed_tokens)\n",
    "#     rels = [\"<UNK>\"]*len(indexed_tokens)\n",
    "# else:\n",
    "#     continue\n",
    "randint = np.random.randint(0,len(testset))\n",
    "biased2 = testset[randint]\n",
    "punctuation = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "for punc in punctuation:\n",
    "    biased2 = biased2.replace(punc,\"\")\n",
    "indexed_tokens = tokenizer.encode(biased2.strip().replace(\" ##\",\"\"), add_special_tokens=True)\n",
    "pos = \" \".join([\"<UNK>\"]*len(indexed_tokens))\n",
    "rels = \" \".join([\"<UNK>\"]*len(indexed_tokens))\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "try: \n",
    "    segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "except:\n",
    "    segments_ids = [0] * (lengths[0] + 1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "rel_tensor = torch.tensor([REL2ID[x] for x in rels.strip().split(\" \")])\n",
    "pos_tensor = torch.tensor([POS2ID[x] for x in pos.strip().split(\" \")])\n",
    "while rel_tensor.size()[0] < len(indexed_tokens):\n",
    "    rel_tensor = torch.cat((rel_tensor, torch.tensor([0])), dim=-1)\n",
    "while pos_tensor.size()[0] < len(indexed_tokens):\n",
    "    pos_tensor = torch.cat((pos_tensor, torch.tensor([0])), dim=-1)\n",
    "\n",
    "# ground_truth_list = [(x[0],len(x[1])) for x in diff(biased2.strip().split(),nonbiased2.strip().split())]\n",
    "# ground_truth = []\n",
    "# for j in ground_truth_list:\n",
    "#     if j[0] == \"-\":\n",
    "#         ground_truth.extend([1]*j[1])\n",
    "#     elif j[0] == \"=\":\n",
    "#         ground_truth.extend([0]*j[1])\n",
    "\n",
    "#print((tokens_tensor,segments_tensors,rel_tensor,pos_tensor))\n",
    "i = i + 1\n",
    "\n",
    "results = model(tokens_tensor,train=\"test\",token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "_,token_logits = results\n",
    "\n",
    "#optimizer.zero_grad()\n",
    "#while len(ground_truth) + 2 < len(softmax(token_logits)[0]):\n",
    "#    ground_truth.extend([0])\n",
    "#loss = celossfn(softmax(token_logits)[0],torch.Tensor([0] + ground_truth + [0]).type('torch.LongTensor'))\n",
    "#loss.backward()\n",
    "#optimizer.step()\n",
    "#    print(loss)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40,\n",
       " 'swallow vast swat',\n",
       " 3.28825e-11,\n",
       " array([2.8172333e-11, 3.1839944e-11, 3.1804011e-11, 3.0874640e-11,\n",
       "        3.2444901e-11, 3.1709156e-11, 2.8031579e-11, 3.2362328e-11,\n",
       "        3.0273024e-11, 2.8587887e-11, 3.0818684e-11, 3.1879322e-11,\n",
       "        2.4084942e-11, 3.1283177e-11, 3.1244122e-11, 2.9364910e-11,\n",
       "        2.8254182e-11, 2.8783653e-11, 2.9082091e-11, 2.9084974e-11,\n",
       "        3.1002988e-11, 3.1080787e-11, 3.0946152e-11, 2.8829475e-11,\n",
       "        3.0931990e-11, 3.2323220e-11, 2.9768799e-11, 3.1943535e-11,\n",
       "        3.1573306e-11, 2.8600268e-11, 2.8017092e-11, 2.8648913e-11,\n",
       "        3.0788077e-11, 3.2268271e-11, 3.0120278e-11, 3.1218268e-11,\n",
       "        3.1325033e-11, 2.5358883e-11, 2.8188403e-11, 2.8942753e-11,\n",
       "        3.2882499e-11, 3.0950166e-11, 3.1015999e-11, 3.0030905e-11,\n",
       "        2.9938711e-11], dtype=float32),\n",
       " array([0.46460515, 0.88149494, 0.87741053, 0.77177083, 0.95025915,\n",
       "        0.8666286 , 0.44860598, 0.94087327, 0.70338637, 0.5118404 ,\n",
       "        0.7654105 , 0.885971  , 0.        , 0.81820846, 0.8137691 ,\n",
       "        0.6001629 , 0.4739088 , 0.53409266, 0.5680155 , 0.5683432 ,\n",
       "        0.78635997, 0.7952032 , 0.7798995 , 0.5393012 , 0.7782897 ,\n",
       "        0.936428  , 0.64607215, 0.8932699 , 0.8511868 , 0.5132477 ,\n",
       "        0.44695932, 0.5187771 , 0.7619314 , 0.93018204, 0.68602407,\n",
       "        0.8108303 , 0.8229661 , 0.14480624, 0.46643186, 0.55217725,\n",
       "        1.        , 0.78035575, 0.7878388 , 0.67586523, 0.6653858 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = softmax(token_logits)[:,:,1][0][1:-1].detach().numpy()\n",
    "idx = np.argmax(index)\n",
    "(idx, tokenizer.decode(indexed_tokens[idx:idx+3]), np.max(index), index, (index-np.min(index))/(np.max(index)-np.min(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952\n",
      "Research by Cornell University has led to the dire prediction, which could see two billion people displaced by 2100. Those who live on coastline or islands are most at risk from the rising sea levels, which threaten to swallow vast swathes of land.\n",
      "('vast', 'swallow vast swat', 3.28825e-11, 1.0)\n",
      "('has', 'university has led', 3.24449e-11, 0.95025915)\n",
      "('the', 'to the dire', 3.2362328e-11, 0.94087327)\n",
      "('or', 'coastline or islands', 3.232322e-11, 0.936428)\n",
      "('rising', 'the rising sea', 3.226827e-11, 0.93018204)\n",
      "('are', 'islands are most', 3.1943535e-11, 0.8932699)\n",
      "('could', 'which could see', 3.1879322e-11, 0.885971)\n",
      "('by', 'research by cornell', 3.1839944e-11, 0.88149494)\n",
      "('cornell', 'by cornell university', 3.180401e-11, 0.87741053)\n",
      "('led', 'has led to', 3.1709156e-11, 0.8666286)\n",
      "('most', 'are most at', 3.1573306e-11, 0.8511868)\n",
      "('which', 'levels which threaten', 3.1325033e-11, 0.8229661)\n",
      "('two', 'see two billion', 3.1283177e-11, 0.81820846)\n",
      "('billion', 'two billion people', 3.1244122e-11, 0.8137691)\n",
      "('levels', 'sea levels which', 3.1218268e-11, 0.8108303)\n",
      "('who', 'those who live', 3.1080787e-11, 0.7952032)\n",
      "('##hes', 'swathes of', 3.1016e-11, 0.7878388)\n",
      "('those', '##0 those who', 3.100299e-11, 0.78635997)\n",
      "('swat', 'vast swathes', 3.0950166e-11, 0.78035575)\n",
      "('live', 'who live on', 3.0946152e-11, 0.7798995)\n",
      "('coastline', 'on coastline or', 3.093199e-11, 0.7782897)\n",
      "('university', 'cornell university has', 3.087464e-11, 0.77177083)\n",
      "('which', 'prediction which could', 3.0818684e-11, 0.7654105)\n",
      "('the', 'from the rising', 3.0788077e-11, 0.7619314)\n",
      "('dire', 'the dire prediction', 3.0273024e-11, 0.70338637)\n",
      "('sea', 'rising sea levels', 3.0120278e-11, 0.68602407)\n",
      "('of', '##hes of land', 3.0030905e-11, 0.67586523)\n",
      "('land', 'of land [SEP]', 2.993871e-11, 0.6653858)\n",
      "('islands', 'or islands are', 2.97688e-11, 0.64607215)\n",
      "('people', 'billion people displaced', 2.936491e-11, 0.6001629)\n",
      "('##0', '2100 those', 2.9084974e-11, 0.5683432)\n",
      "('210', 'by 2100', 2.9082091e-11, 0.5680155)\n",
      "('swallow', 'to swallow vast', 2.8942753e-11, 0.55217725)\n",
      "('on', 'live on coastline', 2.8829475e-11, 0.5393012)\n",
      "('by', 'displaced by 210', 2.8783653e-11, 0.53409266)\n",
      "('from', 'risk from the', 2.8648913e-11, 0.5187771)\n",
      "('at', 'most at risk', 2.8600268e-11, 0.5132477)\n",
      "('prediction', 'dire prediction which', 2.8587887e-11, 0.5118404)\n",
      "('displaced', 'people displaced by', 2.8254182e-11, 0.4739088)\n",
      "('to', 'threaten to swallow', 2.8188403e-11, 0.46643186)\n",
      "('research', '[CLS] research by', 2.8172333e-11, 0.46460515)\n",
      "('to', 'led to the', 2.8031579e-11, 0.44860598)\n",
      "('risk', 'at risk from', 2.8017092e-11, 0.44695932)\n",
      "('threaten', 'which threaten to', 2.5358883e-11, 0.14480624)\n",
      "('see', 'could see two', 2.4084942e-11, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print(randint)\n",
    "print(testset[randint])\n",
    "normindex = (index-np.min(index))/(np.max(index)-np.min(index))\n",
    "for idx in np.flip(np.argsort(index)):\n",
    "    print((tokenizer.decode(indexed_tokens[idx+1:idx+2]),tokenizer.decode(indexed_tokens[idx:idx+3]),index[idx],normindex[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0141e-11, 3.0005e-11, 3.0548e-11, 3.0557e-11, 2.9871e-11, 2.9863e-11,\n",
       "        3.0578e-11, 3.0578e-11, 3.0000e-11, 2.8747e-11, 2.8751e-11, 3.0352e-11,\n",
       "        2.8743e-11, 2.8807e-11, 3.0011e-11], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(token_logits)[:,:,1][0][:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 1), ('+', 1), ('=', 4)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the stand ##out tracks , \" parker \\' s band , \" was a tribute to legendary jazz saxophonist charlie parker .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biased2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'taking of vie ##nti ##ane'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonbiased2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fall of vientiane'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2028,\n",
       " 1997,\n",
       " 1996,\n",
       " 3233,\n",
       " 2041,\n",
       " 3162,\n",
       " 1010,\n",
       " 1000,\n",
       " 6262,\n",
       " 1005,\n",
       " 1055,\n",
       " 2316,\n",
       " 1010,\n",
       " 1000,\n",
       " 2001,\n",
       " 1037,\n",
       " 7050,\n",
       " 2000,\n",
       " 8987,\n",
       " 4166,\n",
       " 19977,\n",
       " 4918,\n",
       " 6262,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(biased2.strip().replace(\"#\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one of the standout tracks , \" parker \\' s band , \" was a tribute to legendary jazz saxophonist charlie parker .'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biased2.replace(\" ##\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'stand',\n",
       " '##out',\n",
       " 'tracks',\n",
       " ',',\n",
       " '\"',\n",
       " 'parker',\n",
       " \"'\",\n",
       " 's',\n",
       " 'band',\n",
       " ',',\n",
       " '\"',\n",
       " 'was',\n",
       " 'a',\n",
       " 'tribute',\n",
       " 'to',\n",
       " 'legendary',\n",
       " 'jazz',\n",
       " 'saxophonist',\n",
       " 'charlie',\n",
       " 'parker',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biased2.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
