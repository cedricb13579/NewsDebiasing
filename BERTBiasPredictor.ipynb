{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "## TAKEN FROM GITHUB CODE OF NEUTRALIZING BIAS\n",
    "## https://github.com/rpryzant/neutralizing-bias\n",
    "\n",
    "# from https://spacy.io/api/annotation#section-dependency-parsing\n",
    "RELATIONS = [\n",
    "    'det', 'amod', 'nsubj', 'prep', 'pobj', 'ROOT', \n",
    "    'attr', 'punct', 'advmod', 'compound', 'acl', 'agent', \n",
    "    'aux', 'ccomp', 'dobj', 'cc', 'conj', 'appos', 'nsubjpass', \n",
    "    'auxpass', 'poss', 'nummod', 'nmod', 'relcl', 'mark', \n",
    "    'advcl', 'pcomp', 'npadvmod', 'preconj', 'neg', 'xcomp', \n",
    "    'csubj', 'prt', 'parataxis', 'expl', 'case', 'acomp', 'predet',\n",
    "    'quantmod', 'dep', 'oprd', 'intj', 'dative', 'meta', 'csubjpass', \n",
    "    '<UNK>'\n",
    "]\n",
    "REL2ID = {x: i for i, x in enumerate(RELATIONS)}\n",
    "\n",
    "# from https://spacy.io/api/annotation#section-pos-tagging\n",
    "POS_TAGS = [\n",
    "    'DET', 'ADJ', 'NOUN', 'ADP', 'NUM', 'VERB', 'PUNCT', 'ADV', \n",
    "    'PART', 'CCONJ', 'PRON', 'X', 'INTJ', 'PROPN', 'SYM',\n",
    "    '<UNK>'\n",
    "]\n",
    "POS2ID = {x: i for i, x in enumerate(POS_TAGS)}\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "class Featurizer:\n",
    "\n",
    "    def __init__(self, tok2id={}, pad_id=0, lexicon_feature_bits=1):\n",
    "        self.tok2id = tok2id\n",
    "        self.id2tok = {x: tok for tok, x in tok2id.items()}\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.pos2id = POS2ID\n",
    "        self.rel2id = REL2ID\n",
    "\n",
    "        self.lexicons = {\n",
    "            'assertives': self.read_lexicon('lexicons/assertives_hooper1975.txt'),\n",
    "            'entailed_arg': self.read_lexicon('lexicons/entailed_arg_berant2012.txt'),\n",
    "            'entailed': self.read_lexicon('lexicons/entailed_berant2012.txt'), \n",
    "            'entailing_arg': self.read_lexicon('lexicons/entailing_arg_berant2012.txt'), \n",
    "            'entailing': self.read_lexicon('lexicons/entailing_berant2012.txt'), \n",
    "            'factives': self.read_lexicon('lexicons/factives_hooper1975.txt'),\n",
    "            'hedges': self.read_lexicon('lexicons/hedges_hyland2005.txt'),\n",
    "            'implicatives': self.read_lexicon('lexicons/implicatives_karttunen1971.txt'),\n",
    "            'negatives': self.read_lexicon('lexicons/negative_liu2005.txt'),\n",
    "            'positives': self.read_lexicon('lexicons/positive_liu2005.txt'),\n",
    "            'npov': self.read_lexicon('lexicons/npov_lexicon.txt'),\n",
    "            'reports': self.read_lexicon('lexicons/report_verbs.txt'),\n",
    "            'strong_subjectives': self.read_lexicon('lexicons/strong_subjectives_riloff2003.txt'),\n",
    "            'weak_subjectives': self.read_lexicon('lexicons/weak_subjectives_riloff2003.txt')\n",
    "        }\n",
    "        self.lexicon_feature_bits = lexicon_feature_bits\n",
    "\n",
    "\n",
    "    def get_feature_names(self):\n",
    "\n",
    "        lexicon_feature_names = list(self.lexicons.keys())\n",
    "        context_feature_names = [x + '_context' for x in lexicon_feature_names]\n",
    "        pos_names = list(list(zip(*sorted(self.pos2id.items(), key=lambda x: x[1])))[0])\n",
    "        rel_names = list(list(zip(*sorted(self.rel2id.items(), key=lambda x: x[1])))[0])\n",
    "\n",
    "        return lexicon_feature_names + context_feature_names + pos_names + rel_names        \n",
    "\n",
    "    def read_lexicon(self, fp):\n",
    "        out = set([\n",
    "            l.strip() for l in open(fp, errors='ignore') \n",
    "            if not l.startswith('#') and not l.startswith(';')\n",
    "            and len(l.strip().split()) == 1\n",
    "        ])\n",
    "        return out\n",
    "\n",
    "\n",
    "    def lexicon_features(self, words, bits=2):\n",
    "        assert bits in [1, 2]\n",
    "        if bits == 1:\n",
    "            true = 1\n",
    "            false = 0\n",
    "        else:\n",
    "            true = [1, 0]\n",
    "            false = [0, 1]\n",
    "    \n",
    "        out = []\n",
    "        for word in words:\n",
    "            out.append([\n",
    "                true if word in lexicon else false \n",
    "                for _, lexicon in self.lexicons.items()\n",
    "            ])\n",
    "        out = np.array(out)\n",
    "\n",
    "        if bits == 2:\n",
    "            out = out.reshape(len(words), -1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def context_features(self, lex_feats, window_size=2):\n",
    "        out = []\n",
    "        nwords = lex_feats.shape[0]\n",
    "        nfeats = lex_feats.shape[1]\n",
    "        for wi in range(lex_feats.shape[0]):\n",
    "            window_start = max(wi - window_size, 0)\n",
    "            window_end = min(wi + window_size + 1, nwords)\n",
    "\n",
    "            left = lex_feats[window_start: wi, :] if wi > 0 else np.zeros((1, nfeats))\n",
    "            right = lex_feats[wi + 1: window_end, :] if wi < nwords - 1 else np.zeros((1, nfeats))\n",
    "\n",
    "            out.append((np.sum(left + right, axis=0) > 0).astype(int))\n",
    "\n",
    "        return np.array(out)\n",
    "\n",
    "\n",
    "    def features(self, id_seq, rel_ids, pos_ids):\n",
    "        if self.pad_id in id_seq:\n",
    "            pad_idx = id_seq.index(self.pad_id)\n",
    "            pad_len = len(id_seq[pad_idx:])\n",
    "            id_seq = id_seq[:pad_idx]\n",
    "            rel_ids = rel_ids[:pad_idx]\n",
    "            pos_ids = pos_ids[:pad_idx]\n",
    "        else:\n",
    "            pad_len = 0\n",
    "\n",
    "        toks = [self.id2tok[x] for x in id_seq]\n",
    "        # build list of [word, [tok indices the word came from]]\n",
    "        words = []\n",
    "        word_indices = []\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.startswith('##'):\n",
    "                words[-1] += tok.replace('##', '')\n",
    "                word_indices[-1].append(i)\n",
    "            else:\n",
    "                words.append(tok)\n",
    "                word_indices.append([i])\n",
    "\n",
    "        # get expert features\n",
    "        lex_feats = self.lexicon_features(words, bits=self.lexicon_feature_bits)\n",
    "        context_feats = self.context_features(lex_feats)\n",
    "        expert_feats = np.concatenate((lex_feats, context_feats), axis=1)\n",
    "        # break word-features into tokens\n",
    "        feats = np.concatenate([\n",
    "            np.repeat(np.expand_dims(word_vec, axis=0), len(indices), axis=0) \n",
    "            for (word_vec, indices) in zip(expert_feats, word_indices)\n",
    "        ], axis=0)\n",
    "\n",
    "        # add in the pos and relational features\n",
    "        pos_feats = np.zeros((len(pos_ids), len(POS2ID)))\n",
    "        pos_feats[range(len(pos_ids)), pos_ids] = 1\n",
    "        rel_feats = np.zeros((len(rel_ids), len(REL2ID)))\n",
    "        rel_feats[range(len(rel_ids)), rel_ids] = 1\n",
    "        \n",
    "        feats = np.concatenate((feats, pos_feats, rel_feats), axis=1)\n",
    "\n",
    "        # add pad back in                \n",
    "        feats = np.concatenate((feats, np.zeros((pad_len, feats.shape[1]))))\n",
    "\n",
    "        return feats\n",
    "\n",
    "\n",
    "    def featurize_batch(self, batch_ids, rel_ids, pos_ids, padded_len=0):\n",
    "        \"\"\" takes [batch, len] returns [batch, len, features] \"\"\"\n",
    "        print(rel_ids)\n",
    "        print(batch_ids)\n",
    "        print(pos_ids)\n",
    "        batch_feats = [\n",
    "            self.features(list(id_seq), list(rel_ids), list(pos_ids)) \n",
    "            for id_seq, rel_ids, pos_ids in zip(batch_ids, rel_ids, pos_ids)]\n",
    "        batch_feats = np.array(batch_feats)\n",
    "        return batch_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok2id = tokenizer.vocab\n",
    "tok2id['<del>'] = len(tok2id)\n",
    "bertmodel = BertModel.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 12\n",
    "hidden_size = 768\n",
    "\n",
    "## TAKEN FROM GITHUB CODE OF NEUTRALIZING BIAS\n",
    "## https://github.com/rpryzant/neutralizing-bias\n",
    "\n",
    "class ConcatCombine(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, feature_size, out_size, layers,\n",
    "            dropout_prob, small=False, pre_enrich=False, activation=False,\n",
    "            include_categories=False, category_emb=False,\n",
    "            add_category_emb=False):\n",
    "        super(ConcatCombine, self).__init__()\n",
    "\n",
    "        self.include_categories = include_categories\n",
    "        self.add_category_emb = add_category_emb\n",
    "        if include_categories:\n",
    "            if category_emb and not add_category_emb:\n",
    "                feature_size *= 2\n",
    "            elif not category_emb:\n",
    "                feature_size += 43\n",
    "\n",
    "        if layers == 1:\n",
    "            self.out = torch.nn.Sequential(\n",
    "                torch.nn.Linear(hidden_size + feature_size, out_size),\n",
    "                torch.nn.Dropout(dropout_prob))\n",
    "        elif layers == 2:\n",
    "            waist_size = min(hidden_size, feature_size) if small else max(hidden_size, feature_size)\n",
    "            if activation:\n",
    "                self.out = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_size + feature_size, waist_size),\n",
    "                    torch.nn.Dropout(dropout_prob),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(waist_size, out_size),\n",
    "                    torch.nn.Dropout(dropout_prob))\n",
    "            else:\n",
    "                self.out = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(hidden_size + feature_size, waist_size),\n",
    "                    torch.nn.Dropout(dropout_prob),\n",
    "                    torch.nn.Linear(waist_size, out_size),\n",
    "                    torch.nn.Dropout(dropout_prob))\n",
    "        if pre_enrich:\n",
    "            if activation:\n",
    "                self.enricher = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(feature_size, feature_size),\n",
    "                    torch.nn.ReLU())\n",
    "            else:\n",
    "                self.enricher = torch.nn.Linear(feature_size, feature_size)\n",
    "        else:\n",
    "            self.enricher = None\n",
    "        # manually set cuda because module doesn't see these combiners for bottom \n",
    "#         if CUDA:\n",
    "#             self.out = self.out.cuda()\n",
    "#             if self.enricher: \n",
    "#                 self.enricher = self.enricher.cuda()\n",
    "                \n",
    "    def forward(self, hidden, features, categories=None):\n",
    "        if self.include_categories:\n",
    "            categories = categories.unsqueeze(1)\n",
    "            categories = categories.repeat(1, features.shape[1], 1)\n",
    "            if self.add_category_emb:\n",
    "                features = features + categories\n",
    "            else:\n",
    "                features = torch.cat((features, categories), -1)\n",
    "\n",
    "        if self.enricher is not None:\n",
    "            features = self.enricher(features)\n",
    "\n",
    "        return self.out(torch.cat((hidden, features), dim=-1))\n",
    "    \n",
    "\n",
    "class BertDetector(torch.nn.Module):\n",
    "    def __init__(self,cls_num_labels=2,token_num_labels=2,tok2id=None):\n",
    "        super(BertDetector,self).__init__()\n",
    "        self.bert = bertmodel\n",
    "        self.featurizer = Featurizer(tok2id,lexicon_feature_bits=1)\n",
    "        self.cls_dropout = torch.nn.Dropout(0.15)\n",
    "        self.cls_classifier = torch.nn.Linear(hidden_size,cls_num_labels)\n",
    "        \n",
    "        self.token_dropout = torch.nn.Dropout(0.15)\n",
    "        #self.token_classifier = torch.nn.Linear(hidden_size,token_num_labels)\n",
    "        \n",
    "        self.token_classifier = ConcatCombine(\n",
    "                hidden_size, 90, token_num_labels, \n",
    "                1, 0.15, False, pre_enrich=False,\n",
    "                activation=False,\n",
    "                include_categories=False,\n",
    "                category_emb=False,\n",
    "                add_category_emb=False)\n",
    "        \n",
    "    def forward(self,input_ids,token_type_ids=None,attention_mask=None, \n",
    "        labels=None,rel_ids=None,pos_ids=None,categories=None,pre_len=None):\n",
    "        features = torch.tensor(self.featurizer.featurize_batch(\n",
    "            input_ids.numpy(), \n",
    "            rel_ids.numpy(), \n",
    "            pos_ids.numpy(), \n",
    "            padded_len=input_ids.shape[1]),dtype=torch.float)\n",
    "        results = self.bert(input_ids)\n",
    "        sequence = results.last_hidden_state\n",
    "        pooled = results.pooler_output\n",
    "        cls_logits = self.cls_dropout(self.cls_classifier(pooled))\n",
    "        token_logits = self.token_dropout(self.token_classifier(sequence,features))\n",
    "        return cls_logits,token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101,  2859, 18152,  2006,  9317,  2000,  1000,  2954,  2067,  1000,\n",
       "           2044,  1996,  2142,  2163,  2623,  1037,  2825,  2117,  2461,  1997,\n",
       "          23234, 21857,  2015,  2006,  1002,  3263,  4551,  4276,  1997,  2822,\n",
       "           5350,  1012,  1996,  1057,  1012,  1055,  1012,  6378,  2207,  2006,\n",
       "           9857,  2443,  3445,  7773,  2006, 10964,  2833,  3688,  1998,  7325,\n",
       "           8139,  1012,   102]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0]]),\n",
       " tensor([45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
       "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45,\n",
       "         45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45]),\n",
       " tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1 = \"China vowed on Wednesday to \\\"fight back\\\" after the United States announced a possible second round of tariff hikes on $200 billion worth of Chinese goods. The U.S. proposal released on Tuesday included increased taxes on imported food products and consumer electronics.\"\n",
    "text_2 = \"not sure\"\n",
    "indexed_tokens = tokenizer.encode(text_1, add_special_tokens=True)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "try: \n",
    "    segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "except:\n",
    "    segments_ids = [0] * (lengths[0] + 1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "segments_tensors\n",
    "rel_tensor = torch.tensor([REL2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "pos_tensor = torch.tensor([POS2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "\n",
    "(tokens_tensor,segments_tensors,rel_tensor,pos_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n",
      "  45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45 45\n",
      "  45 45 45 45 45]]\n",
      "[[  101  2859 18152  2006  9317  2000  1000  2954  2067  1000  2044  1996\n",
      "   2142  2163  2623  1037  2825  2117  2461  1997 23234 21857  2015  2006\n",
      "   1002  3263  4551  4276  1997  2822  5350  1012  1996  1057  1012  1055\n",
      "   1012  6378  2207  2006  9857  2443  3445  7773  2006 10964  2833  3688\n",
      "   1998  7325  8139  1012   102]]\n",
      "[[15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      "  15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15\n",
      "  15 15 15 15 15]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[ 0.7067, -0.7842]], grad_fn=<MulBackward0>),\n",
       "  tensor([[[ 0.9394,  0.2291],\n",
       "           [ 0.2739, -0.1885],\n",
       "           [ 0.1409, -0.3158],\n",
       "           [ 0.0000, -0.4369],\n",
       "           [ 0.1004,  0.0000],\n",
       "           [ 0.3477, -0.2110],\n",
       "           [ 0.0000,  0.0000],\n",
       "           [ 0.2604, -0.1540],\n",
       "           [-0.0000, -0.0000],\n",
       "           [ 0.0000,  0.0000],\n",
       "           [ 0.6586, -0.0000],\n",
       "           [ 0.0000, -0.1458],\n",
       "           [ 1.2494, -0.0000],\n",
       "           [ 0.9563,  0.0468],\n",
       "           [ 0.0000, -0.0000],\n",
       "           [-0.0810, -0.4441],\n",
       "           [ 0.1827, -0.5043],\n",
       "           [ 0.3458, -0.3584],\n",
       "           [-0.1947, -0.4056],\n",
       "           [ 0.0000, -0.6572],\n",
       "           [ 0.7294,  0.0107],\n",
       "           [ 0.1171,  0.0090],\n",
       "           [ 0.0000, -0.0000],\n",
       "           [ 0.5165, -0.5907],\n",
       "           [ 0.4840, -0.3296],\n",
       "           [ 0.9114, -0.0000],\n",
       "           [ 0.0403, -0.5425],\n",
       "           [ 0.0000, -0.3294],\n",
       "           [ 0.6837, -0.5323],\n",
       "           [ 0.3622, -0.3345],\n",
       "           [ 0.0000,  0.0157],\n",
       "           [ 0.9954, -0.0000],\n",
       "           [-0.0000,  0.2156],\n",
       "           [ 0.0000, -0.6008],\n",
       "           [ 0.9428, -0.6486],\n",
       "           [ 0.9088,  0.1409],\n",
       "           [ 0.9014, -0.6750],\n",
       "           [-0.2416,  0.2437],\n",
       "           [-0.0667,  0.0000],\n",
       "           [ 0.0035, -0.2968],\n",
       "           [-0.1692, -0.0362],\n",
       "           [ 0.2329, -0.0648],\n",
       "           [ 0.3553,  0.0175],\n",
       "           [ 0.3171,  0.1674],\n",
       "           [ 0.1804, -0.0000],\n",
       "           [ 0.5736, -0.3351],\n",
       "           [ 0.2432, -0.3497],\n",
       "           [ 0.0000, -0.0000],\n",
       "           [ 0.0000, -0.0000],\n",
       "           [ 0.8066, -0.1109],\n",
       "           [ 0.7103, -0.1020],\n",
       "           [ 0.9558, -0.5669],\n",
       "           [-0.0469, -0.2842]]], grad_fn=<MulBackward0>)),\n",
       " tensor([[[0.0326, 0.0275],\n",
       "          [0.0167, 0.0181],\n",
       "          [0.0146, 0.0160],\n",
       "          [0.0127, 0.0141],\n",
       "          [0.0141, 0.0219],\n",
       "          [0.0180, 0.0177],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0165, 0.0188],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0246, 0.0219],\n",
       "          [0.0127, 0.0189],\n",
       "          [0.0444, 0.0219],\n",
       "          [0.0331, 0.0229],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0117, 0.0140],\n",
       "          [0.0153, 0.0132],\n",
       "          [0.0180, 0.0153],\n",
       "          [0.0105, 0.0146],\n",
       "          [0.0127, 0.0113],\n",
       "          [0.0264, 0.0221],\n",
       "          [0.0143, 0.0221],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0213, 0.0121],\n",
       "          [0.0206, 0.0157],\n",
       "          [0.0317, 0.0219],\n",
       "          [0.0132, 0.0127],\n",
       "          [0.0127, 0.0157],\n",
       "          [0.0252, 0.0128],\n",
       "          [0.0183, 0.0157],\n",
       "          [0.0127, 0.0222],\n",
       "          [0.0344, 0.0219],\n",
       "          [0.0127, 0.0271],\n",
       "          [0.0127, 0.0120],\n",
       "          [0.0327, 0.0114],\n",
       "          [0.0316, 0.0252],\n",
       "          [0.0313, 0.0111],\n",
       "          [0.0100, 0.0279],\n",
       "          [0.0119, 0.0219],\n",
       "          [0.0128, 0.0163],\n",
       "          [0.0107, 0.0211],\n",
       "          [0.0161, 0.0205],\n",
       "          [0.0182, 0.0223],\n",
       "          [0.0175, 0.0259],\n",
       "          [0.0152, 0.0219],\n",
       "          [0.0226, 0.0156],\n",
       "          [0.0162, 0.0154],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0127, 0.0219],\n",
       "          [0.0285, 0.0196],\n",
       "          [0.0259, 0.0198],\n",
       "          [0.0331, 0.0124],\n",
       "          [0.0121, 0.0165]]], grad_fn=<SoftmaxBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertDetector(tok2id=tok2id)\n",
    "results = model(tokens_tensor,token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "(results,torch.nn.Softmax(dim=1)(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simplediff import diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(biased.strip().split(),nonbiased.strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  101, 10381, 10626, 11253,  2953,  2213,  1000,  1996,  8382,  2166,\n",
       "           3736,  6299,  1000,  2019,  3720,  2012,  4345,  2118,  4346,  5875,\n",
       "           8866,  2055, 10381, 10626, 11253,  2953,  2213,  1012,   102, 10381,\n",
       "          10626, 11253,  2953,  2213,  1000,  1996,  8382,  2166,  3736,  6299,\n",
       "           1000,  2019,  3720,  2012,  4345,  2118,  4346,  8866,  2055, 10381,\n",
       "          10626, 11253,  2953,  2213,  1012,   102]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " tensor([ 5,  5,  5,  5,  5,  7,  0,  1, 14, 14, 14,  7,  0, 17,  3,  9,  4, 10,\n",
       "          1, 14,  3,  4,  4,  4,  4,  4,  7,  5,  5,  5,  5,  5,  7,  0,  1, 14,\n",
       "         14, 14,  7,  0, 17,  3,  9,  4, 10,  1, 14,  3,  4,  4,  4,  4,  4,  7,\n",
       "          0,  0]),\n",
       " tensor([2, 2, 2, 2, 2, 6, 0, 1, 2, 2, 2, 6, 0, 2, 3, 2, 2, 5, 1, 2, 3, 2, 2, 2,\n",
       "         2, 2, 6, 2, 2, 2, 2, 2, 6, 0, 1, 2, 2, 2, 6, 0, 2, 3, 2, 2, 5, 1, 2, 3,\n",
       "         2, 2, 2, 2, 2, 6, 0, 0]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_test= \"165188319\tch ##lor ##of ##or ##m \\\" the molecular life ##sa ##ver \\\" an article at oxford university providing interesting facts about ch ##lor ##of ##or ##m .\tch ##lor ##of ##or ##m \\\" the molecular life ##sa ##ver \\\" an article at oxford university providing facts about ch ##lor ##of ##or ##m .\tchloroform \\\"the molecular lifesaver\\\" an article at oxford university providing interesting facts about chloroform.\tchloroform \\\"the molecular lifesaver\\\" an article at oxford university providing facts about chloroform.\tNOUN NOUN NOUN NOUN NOUN PUNCT DET ADJ NOUN NOUN NOUN PUNCT DET NOUN ADP NOUN NOUN VERB ADJ NOUN ADP NOUN NOUN NOUN NOUN NOUN PUNCT\tROOT ROOT ROOT ROOT ROOT punct det amod dobj dobj dobj punct det appos prep compound pobj acl amod dobj prep pobj pobj pobj pobj pobj punct\"\n",
    "[revid, _, _, biased, nonbiased, pos, rels] = text_test.strip().split(\"\\t\")\n",
    "\n",
    "indexed_tokens = tokenizer.encode(biased.strip(),nonbiased.strip(), add_special_tokens=True)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "lengths = (tokens_tensor == 102).nonzero(as_tuple=True)[1]\n",
    "try: \n",
    "    segments_ids = [0] * (lengths[0] + 1) + [1] * (lengths[1] - lengths[0])\n",
    "except:\n",
    "    segments_ids = [0] * (lengths[0] + 1)\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "segments_tensors\n",
    "rel_tensor = torch.tensor([REL2ID[x] for x in rels.strip().split(\" \")]*2)\n",
    "pos_tensor = torch.tensor([POS2ID[x] for x in pos.strip().split(\" \")]*2)\n",
    "while rel_tensor.size()[0] < len(indexed_tokens):\n",
    "    rel_tensor = torch.cat((rel_tensor, torch.tensor([0])), dim=-1)\n",
    "while pos_tensor.size()[0] < len(indexed_tokens):\n",
    "    pos_tensor = torch.cat((pos_tensor, torch.tensor([0])), dim=-1)\n",
    "    \n",
    "ground_truth_list = [(x[0],len(x[1])) for x in diff(biased.strip().split(),nonbiased.strip().split())]\n",
    "ground_truth = []\n",
    "for i in ground_truth_list:\n",
    "    if i[0] == \"-\":\n",
    "        ground_truth.extend([1]*i[1])\n",
    "    else:\n",
    "        ground_truth.extend([0]*i[1])\n",
    "ground_truth\n",
    "#rel_tensor = torch.tensor([REL2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "#pos_tensor = torch.tensor([POS2ID[x] for x in [\"<UNK>\"]*len(indexed_tokens)])\n",
    "\n",
    "(tokens_tensor,segments_tensors,rel_tensor,pos_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 56]), torch.Size([1, 56]), torch.Size([56]), torch.Size([56]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tokens_tensor.size(),segments_tensors.size(),rel_tensor.size(),pos_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  5  5  5  5  7  0  1 14 14 14  7  0 17  3  9  4 10  1 14  3  4  4  4\n",
      "   4  4  7  5  5  5  5  5  7  0  1 14 14 14  7  0 17  3  9  4 10  1 14  3\n",
      "   4  4  4  4  4  7  0  0]]\n",
      "[[  101 10381 10626 11253  2953  2213  1000  1996  8382  2166  3736  6299\n",
      "   1000  2019  3720  2012  4345  2118  4346  5875  8866  2055 10381 10626\n",
      "  11253  2953  2213  1012   102 10381 10626 11253  2953  2213  1000  1996\n",
      "   8382  2166  3736  6299  1000  2019  3720  2012  4345  2118  4346  8866\n",
      "   2055 10381 10626 11253  2953  2213  1012   102]]\n",
      "[[2 2 2 2 2 6 0 1 2 2 2 6 0 2 3 2 2 5 1 2 3 2 2 2 2 2 6 2 2 2 2 2 6 0 1 2\n",
      "  2 2 6 0 2 3 2 2 5 1 2 3 2 2 2 2 2 6 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[-0.1879,  0.2159]], grad_fn=<MulBackward0>),\n",
       "  tensor([[[-0.9896, -0.5885],\n",
       "           [-0.0000, -0.1043],\n",
       "           [-0.9724, -0.7916],\n",
       "           [-0.7461, -0.0000],\n",
       "           [-0.0000, -0.3750],\n",
       "           [-0.0000, -0.1544],\n",
       "           [ 0.0614, -0.4089],\n",
       "           [-0.2645, -0.0779],\n",
       "           [-0.6189, -0.0378],\n",
       "           [-0.6264, -0.0609],\n",
       "           [-0.0000, -0.7182],\n",
       "           [-0.8381, -0.4092],\n",
       "           [ 0.2552, -0.3033],\n",
       "           [-0.0000, -0.0000],\n",
       "           [-0.6066, -0.2585],\n",
       "           [-0.1947, -0.0383],\n",
       "           [-0.0000, -0.0000],\n",
       "           [-0.0000, -0.0000],\n",
       "           [-0.1722, -0.0000],\n",
       "           [-0.3849, -0.3014],\n",
       "           [-0.0000, -0.1335],\n",
       "           [-0.7658, -0.2917],\n",
       "           [-0.2750, -0.0000],\n",
       "           [-0.0000, -0.0000],\n",
       "           [-0.4523, -0.0000],\n",
       "           [-0.8212, -0.5299],\n",
       "           [-0.0000, -0.0000],\n",
       "           [ 0.1374, -0.2520],\n",
       "           [-0.3398,  0.7106],\n",
       "           [-0.0000, -0.5433],\n",
       "           [-0.7148, -0.0000],\n",
       "           [-0.4330, -0.0000],\n",
       "           [-0.8630, -0.4607],\n",
       "           [-0.0000, -0.0000],\n",
       "           [ 0.0000, -0.0000],\n",
       "           [-0.0000, -0.0000],\n",
       "           [-0.4333, -0.0175],\n",
       "           [-0.4951, -0.0000],\n",
       "           [-0.8710, -0.6881],\n",
       "           [-0.0000, -0.0000],\n",
       "           [ 0.2882, -0.3290],\n",
       "           [-0.6440, -0.0507],\n",
       "           [-0.4239, -0.2291],\n",
       "           [-0.0000, -0.0154],\n",
       "           [-0.8766, -0.0000],\n",
       "           [-0.4308,  0.0649],\n",
       "           [-0.1169, -0.4303],\n",
       "           [-0.6617, -0.1657],\n",
       "           [-0.8009, -0.2431],\n",
       "           [-0.0000, -0.6963],\n",
       "           [-0.7405, -0.0000],\n",
       "           [-0.4499, -0.9870],\n",
       "           [-0.0000, -0.5423],\n",
       "           [-0.0000, -0.0000],\n",
       "           [ 0.1739, -0.4373],\n",
       "           [-0.0000,  0.6788]]], grad_fn=<MulBackward0>)),\n",
       " tensor([[[0.0085, 0.0114],\n",
       "          [0.0228, 0.0184],\n",
       "          [0.0086, 0.0093],\n",
       "          [0.0108, 0.0204],\n",
       "          [0.0228, 0.0141],\n",
       "          [0.0228, 0.0175],\n",
       "          [0.0242, 0.0136],\n",
       "          [0.0175, 0.0189],\n",
       "          [0.0123, 0.0197],\n",
       "          [0.0122, 0.0192],\n",
       "          [0.0228, 0.0100],\n",
       "          [0.0099, 0.0136],\n",
       "          [0.0294, 0.0151],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0124, 0.0158],\n",
       "          [0.0188, 0.0197],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0192, 0.0204],\n",
       "          [0.0155, 0.0151],\n",
       "          [0.0228, 0.0179],\n",
       "          [0.0106, 0.0153],\n",
       "          [0.0173, 0.0204],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0145, 0.0204],\n",
       "          [0.0100, 0.0120],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0261, 0.0159],\n",
       "          [0.0162, 0.0416],\n",
       "          [0.0228, 0.0119],\n",
       "          [0.0112, 0.0204],\n",
       "          [0.0148, 0.0204],\n",
       "          [0.0096, 0.0129],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0148, 0.0201],\n",
       "          [0.0139, 0.0204],\n",
       "          [0.0095, 0.0103],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0304, 0.0147],\n",
       "          [0.0120, 0.0194],\n",
       "          [0.0149, 0.0163],\n",
       "          [0.0228, 0.0201],\n",
       "          [0.0095, 0.0204],\n",
       "          [0.0148, 0.0218],\n",
       "          [0.0203, 0.0133],\n",
       "          [0.0118, 0.0173],\n",
       "          [0.0102, 0.0160],\n",
       "          [0.0228, 0.0102],\n",
       "          [0.0109, 0.0204],\n",
       "          [0.0145, 0.0076],\n",
       "          [0.0228, 0.0119],\n",
       "          [0.0228, 0.0204],\n",
       "          [0.0271, 0.0132],\n",
       "          [0.0228, 0.0403]]], grad_fn=<SoftmaxBackward>),\n",
       " tensor([[1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "          1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "          1, 0, 1, 0, 0, 0, 0, 1]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertDetector(tok2id=tok2id)\n",
    "results = model(tokens_tensor,token_type_ids=segments_tensors,rel_ids=rel_tensor.unsqueeze(0),pos_ids=pos_tensor.unsqueeze(0))\n",
    "(results,torch.nn.Softmax(dim=1)(results[1]),torch.argmax(torch.nn.Softmax(dim=1)(results[1]),dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.9009634 , 1.        , 1.        , 0.6872803 ,\n",
       "        0.8569493 , 0.62477714, 1.        , 1.        , 1.        ,\n",
       "        0.48761815, 1.        , 0.57204586, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8749847 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.67752767, 1.        , 0.58082134,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.5394351 , 1.        , 1.        , 0.98471063, 1.        ,\n",
       "        1.        , 0.730942  , 1.        , 1.        , 0.49842423,\n",
       "        1.        , 0.5844139 , 0.5814122 , 1.        , 0.54270226,\n",
       "        1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(results[1].detach())[:,:,:2]\n",
    "x = x - x.max(axis=2, keepdims=True)\n",
    "y = np.exp(x)\n",
    "y / y.sum(axis=2, keepdims=True)\n",
    "y[:,:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
